{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SXpaKwwGe5x"
      },
      "source": [
        "# TM10007 Group 4 -- ECG Data\n",
        "Lara de Bats (5022037), Josefien van den Berg (4663381), Merel Goossens (4856902), Amber Liqui Lung (4464168)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment we wil test certain classifiers in order to create a classifier model to distinguish between normal and abnormal ECG's. The code is divided in different segments that are explained in our report."
      ],
      "metadata": {
        "id": "u0jPpiE5pyEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading packages\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "import numpy as np\n",
        "import seaborn\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab as pl\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.model_selection import KFold, GridSearchCV, RandomizedSearchCV, train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import f1_score, make_scorer, accuracy_score, roc_auc_score\n",
        "# from sklearn.pipeline import Pipeline\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn import set_config\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "pd.set_option('display.max_rows', 30)"
      ],
      "metadata": {
        "id": "f-5Aw7sQpsDL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EG78VSHfhNi"
      },
      "source": [
        "## 1. Data loading, initial split, exploring and cleaning\n",
        "\n",
        "Below we load the ECG data, convert it to a dataframe an clean the data.First we are going to start with loading the CS file into a dataframe. Then we are going to split the dataset into test data and training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CiDn2Sk-VWqE",
        "outputId": "6a5147d5-7132-4e70-f515-89b028bdcf23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TM10007_ML_ECG_group4'...\n",
            "remote: Enumerating objects: 403, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (61/61), done.\u001b[K\n",
            "remote: Total 403 (delta 35), reused 15 (delta 5), pack-reused 337\u001b[K\n",
            "Receiving objects: 100% (403/403), 95.72 MiB | 37.10 MiB/s, done.\n",
            "Resolving deltas: 100% (215/215), done.\n",
            "The number of samples: 827\n",
            "The number of columns: 9001\n"
          ]
        }
      ],
      "source": [
        "# Load CSV file into dataframe\n",
        "!git clone https://github.com/JosefienBerg/TM10007_ML_ECG_group4.git\n",
        "\n",
        "with zipfile.ZipFile('/content/TM10007_ML_ECG_group4/ecg/ecg_data.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/TM10007_ML_ECG_group4/ecg')\n",
        "\n",
        "df = pd.read_csv('/content/TM10007_ML_ECG_group4/ecg/ecg_data.csv', index_col=0)\n",
        "\n",
        "print(f'The number of samples: {len(df.index)}')\n",
        "print(f'The number of columns: {len(df.columns)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Underneath we explore our dataset"
      ],
      "metadata": {
        "id": "OaN6z_VynlYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many patients have a normal or abnormal ECG?\n",
        "count0= (df['label'] == 0).sum()\n",
        "count1= (df['label'] == 1).sum()\n",
        "print(f'There are {count0} patients with label 0')\n",
        "print(f'There are {count1} patients with label 1')\n",
        "# Since the majority of patients has no abnormalities we can conclude that label 0 is normal and label 1 is abnormal "
      ],
      "metadata": {
        "id": "6uzs9dYjnrHt",
        "outputId": "1ce38219-1cea-47be-a30f-fadffbd72c9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 681 patients with label 0\n",
            "There are 146 patients with label 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will perform our initial split"
      ],
      "metadata": {
        "id": "ORtoTNM7LF54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the whole data set in a training and test set\n",
        "features = df.loc[:, df.columns !=\"label\"].to_numpy()\n",
        "labels = df[\"label\"].to_numpy()\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split (features, labels, train_size = 0.8, random_state = 42) "
      ],
      "metadata": {
        "id": "7WHZbHF0LKCk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets explore x_train"
      ],
      "metadata": {
        "id": "jhZt3EF9Wf3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at the size\n",
        "print(f\"The training set has {x_train.shape[0]} samples and {x_train.shape[1]} features with {y_train.shape[0]} labels\")\n",
        "print(f\"The test has {x_test.shape[0]} samples and {x_test.shape[1]} features with {y_test.shape[0]} labels\")\n",
        "\n",
        "# Look at the number of label 1 and label 0 in trainingset\n",
        "count0= (y_train == 0).sum()\n",
        "count1= (y_train == 1).sum()\n",
        "print(f'There are {count0} patients with label 0')\n",
        "print(f'There are {count1} patients with label 1')\n",
        "\n",
        "# Look at the distribution\n",
        "df = pd.DataFrame(x_train)\n",
        "display(df.describe())\n",
        "\n",
        "# Look at the type of data\n",
        "print(x_train.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "g3RxneYdWh2Y",
        "outputId": "05dbaa34-18ba-4fde-9700-d885528aa0f6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The training set has 661 samples and 9000 features with 661 labels\n",
            "The test has 166 samples and 9000 features with 166 labels\n",
            "There are 543 patients with label 0\n",
            "There are 118 patients with label 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "              0            1           2           3            4     \\\n",
              "count   661.000000   661.000000  661.000000  661.000000   661.000000   \n",
              "mean    473.545787    68.219735   40.107831   30.223831    32.912681   \n",
              "std    1090.597625   183.150669   82.932772   56.989313    58.226109   \n",
              "min       0.031717     0.338026    0.374062    0.392388     0.442473   \n",
              "25%       4.048258     6.697330    7.502202    7.973962     9.699800   \n",
              "50%      11.301568    14.319638   16.019461   15.585303    18.526251   \n",
              "75%     414.437620    47.007877   36.407946   29.570521    34.987714   \n",
              "max    9510.877805  2022.415412  963.283307  776.631173  1016.707977   \n",
              "\n",
              "             5            6           7           8           9     ...  \\\n",
              "count  661.000000   661.000000  661.000000  661.000000  661.000000  ...   \n",
              "mean    42.555519    54.601058   38.398261   27.756282   22.688850  ...   \n",
              "std     46.396159    61.963314   52.542619   41.189322   36.309774  ...   \n",
              "min      0.468230     0.291254    0.181994    0.168997    0.732216  ...   \n",
              "25%     13.513050    17.151425   11.714197    8.290047    7.268737  ...   \n",
              "50%     26.475948    39.914146   24.261486   15.910253   13.764587  ...   \n",
              "75%     53.298597    77.536913   48.212765   30.232692   25.157698  ...   \n",
              "max    443.762983  1051.557893  775.236262  483.852043  548.180243  ...   \n",
              "\n",
              "             8990        8991        8992        8993        8994        8995  \\\n",
              "count  661.000000  661.000000  661.000000  661.000000  661.000000  661.000000   \n",
              "mean     0.257628    0.256989    0.260101    0.261333    0.258008    0.259235   \n",
              "std      0.466041    0.492337    0.453205    0.485969    0.523492    0.561868   \n",
              "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
              "25%      0.083260    0.077778    0.085630    0.083037    0.077072    0.084123   \n",
              "50%      0.142454    0.140854    0.148017    0.139903    0.139145    0.132906   \n",
              "75%      0.254538    0.242179    0.261747    0.250637    0.241596    0.242516   \n",
              "max      6.808234    7.650954    8.154244    8.203337    8.772557    8.889082   \n",
              "\n",
              "             8996        8997        8998        8999  \n",
              "count  661.000000  661.000000  661.000000  661.000000  \n",
              "mean     0.258205    0.251576    0.259956    0.258827  \n",
              "std      0.473904    0.472443    0.533672    0.534501  \n",
              "min      0.000000    0.000000    0.000000    0.000000  \n",
              "25%      0.084571    0.079911    0.074682    0.078336  \n",
              "50%      0.141514    0.137632    0.141386    0.132081  \n",
              "75%      0.260533    0.253220    0.249928    0.244466  \n",
              "max      9.048367    9.013666    9.058323    9.035147  \n",
              "\n",
              "[8 rows x 9000 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eaddf55f-72fb-4ce3-90af-90fac62b23b3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>8990</th>\n",
              "      <th>8991</th>\n",
              "      <th>8992</th>\n",
              "      <th>8993</th>\n",
              "      <th>8994</th>\n",
              "      <th>8995</th>\n",
              "      <th>8996</th>\n",
              "      <th>8997</th>\n",
              "      <th>8998</th>\n",
              "      <th>8999</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>473.545787</td>\n",
              "      <td>68.219735</td>\n",
              "      <td>40.107831</td>\n",
              "      <td>30.223831</td>\n",
              "      <td>32.912681</td>\n",
              "      <td>42.555519</td>\n",
              "      <td>54.601058</td>\n",
              "      <td>38.398261</td>\n",
              "      <td>27.756282</td>\n",
              "      <td>22.688850</td>\n",
              "      <td>...</td>\n",
              "      <td>0.257628</td>\n",
              "      <td>0.256989</td>\n",
              "      <td>0.260101</td>\n",
              "      <td>0.261333</td>\n",
              "      <td>0.258008</td>\n",
              "      <td>0.259235</td>\n",
              "      <td>0.258205</td>\n",
              "      <td>0.251576</td>\n",
              "      <td>0.259956</td>\n",
              "      <td>0.258827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1090.597625</td>\n",
              "      <td>183.150669</td>\n",
              "      <td>82.932772</td>\n",
              "      <td>56.989313</td>\n",
              "      <td>58.226109</td>\n",
              "      <td>46.396159</td>\n",
              "      <td>61.963314</td>\n",
              "      <td>52.542619</td>\n",
              "      <td>41.189322</td>\n",
              "      <td>36.309774</td>\n",
              "      <td>...</td>\n",
              "      <td>0.466041</td>\n",
              "      <td>0.492337</td>\n",
              "      <td>0.453205</td>\n",
              "      <td>0.485969</td>\n",
              "      <td>0.523492</td>\n",
              "      <td>0.561868</td>\n",
              "      <td>0.473904</td>\n",
              "      <td>0.472443</td>\n",
              "      <td>0.533672</td>\n",
              "      <td>0.534501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.031717</td>\n",
              "      <td>0.338026</td>\n",
              "      <td>0.374062</td>\n",
              "      <td>0.392388</td>\n",
              "      <td>0.442473</td>\n",
              "      <td>0.468230</td>\n",
              "      <td>0.291254</td>\n",
              "      <td>0.181994</td>\n",
              "      <td>0.168997</td>\n",
              "      <td>0.732216</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>4.048258</td>\n",
              "      <td>6.697330</td>\n",
              "      <td>7.502202</td>\n",
              "      <td>7.973962</td>\n",
              "      <td>9.699800</td>\n",
              "      <td>13.513050</td>\n",
              "      <td>17.151425</td>\n",
              "      <td>11.714197</td>\n",
              "      <td>8.290047</td>\n",
              "      <td>7.268737</td>\n",
              "      <td>...</td>\n",
              "      <td>0.083260</td>\n",
              "      <td>0.077778</td>\n",
              "      <td>0.085630</td>\n",
              "      <td>0.083037</td>\n",
              "      <td>0.077072</td>\n",
              "      <td>0.084123</td>\n",
              "      <td>0.084571</td>\n",
              "      <td>0.079911</td>\n",
              "      <td>0.074682</td>\n",
              "      <td>0.078336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>11.301568</td>\n",
              "      <td>14.319638</td>\n",
              "      <td>16.019461</td>\n",
              "      <td>15.585303</td>\n",
              "      <td>18.526251</td>\n",
              "      <td>26.475948</td>\n",
              "      <td>39.914146</td>\n",
              "      <td>24.261486</td>\n",
              "      <td>15.910253</td>\n",
              "      <td>13.764587</td>\n",
              "      <td>...</td>\n",
              "      <td>0.142454</td>\n",
              "      <td>0.140854</td>\n",
              "      <td>0.148017</td>\n",
              "      <td>0.139903</td>\n",
              "      <td>0.139145</td>\n",
              "      <td>0.132906</td>\n",
              "      <td>0.141514</td>\n",
              "      <td>0.137632</td>\n",
              "      <td>0.141386</td>\n",
              "      <td>0.132081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>414.437620</td>\n",
              "      <td>47.007877</td>\n",
              "      <td>36.407946</td>\n",
              "      <td>29.570521</td>\n",
              "      <td>34.987714</td>\n",
              "      <td>53.298597</td>\n",
              "      <td>77.536913</td>\n",
              "      <td>48.212765</td>\n",
              "      <td>30.232692</td>\n",
              "      <td>25.157698</td>\n",
              "      <td>...</td>\n",
              "      <td>0.254538</td>\n",
              "      <td>0.242179</td>\n",
              "      <td>0.261747</td>\n",
              "      <td>0.250637</td>\n",
              "      <td>0.241596</td>\n",
              "      <td>0.242516</td>\n",
              "      <td>0.260533</td>\n",
              "      <td>0.253220</td>\n",
              "      <td>0.249928</td>\n",
              "      <td>0.244466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>9510.877805</td>\n",
              "      <td>2022.415412</td>\n",
              "      <td>963.283307</td>\n",
              "      <td>776.631173</td>\n",
              "      <td>1016.707977</td>\n",
              "      <td>443.762983</td>\n",
              "      <td>1051.557893</td>\n",
              "      <td>775.236262</td>\n",
              "      <td>483.852043</td>\n",
              "      <td>548.180243</td>\n",
              "      <td>...</td>\n",
              "      <td>6.808234</td>\n",
              "      <td>7.650954</td>\n",
              "      <td>8.154244</td>\n",
              "      <td>8.203337</td>\n",
              "      <td>8.772557</td>\n",
              "      <td>8.889082</td>\n",
              "      <td>9.048367</td>\n",
              "      <td>9.013666</td>\n",
              "      <td>9.058323</td>\n",
              "      <td>9.035147</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows Ã— 9000 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eaddf55f-72fb-4ce3-90af-90fac62b23b3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-eaddf55f-72fb-4ce3-90af-90fac62b23b3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-eaddf55f-72fb-4ce3-90af-90fac62b23b3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a last step, we want do delelte entire rows and columns with only zeros or NaN as that can be seen as missing data."
      ],
      "metadata": {
        "id": "We8ePYjT1Qid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Delete rows and columns with only zero's or NaN's\n",
        "def delete_missing_rows(x, y):\n",
        "  concat = np.c_[x, y]\n",
        "\n",
        "  for row in x:\n",
        "    index = np.where((row == 0).all) or np.where((row == np.nan).all)\n",
        "    np.delete(concat, index, 0)\n",
        "  \n",
        "  x =  concat[:,:-1]\n",
        "  y = concat[:,-1] \n",
        "  return x, y\n",
        "\n",
        "x_train, y_train = delete_missing_rows(x_train, y_train)"
      ],
      "metadata": {
        "id": "zuqOeZIQ1FSS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data preprocessing - creating pipelines\n",
        "Datasets with missing values that are filled with placeholders such as NaN and None, can cause problems when using estimators. Therefore you want to perform imputation in order to create usable datasets."
      ],
      "metadata": {
        "id": "hbyr7EH1fUE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Underneath we create the functions for the imputation steps"
      ],
      "metadata": {
        "id": "iiggNmNvmsOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #Replace nan's and zeros since the chance of a zero is very low due to only float types\n",
        "# def replace_missing_values(x): \n",
        "#   imp = SimpleImputer(missing_values = np.nan, strategy='mean', copy = False)\n",
        "#   imp.fit_transform(x, y=None) #removes entire columns if it contains only missing values\n",
        "\n",
        "#   imp = SimpleImputer(missing_values = 0, strategy='mean', copy = False)\n",
        "#   imp.fit_transform(x, y=None) #removes entire columns if it contains only missing values\n",
        "#   return x\n",
        "\n",
        "# # Call functions to perform imputation steps and rename x_train\n",
        "# x_train_imp = replace_missing_values(x_train)\n",
        "\n",
        "# count0= (x_train_imp == 0).sum() \n",
        "# countNaN = np.isnan(x_train_imp)[np.isnan(x_train_imp) == True].size\n",
        "# print(f\"There are {count0} zeros and {countNaN} NaN values left in the dataset\")"
      ],
      "metadata": {
        "id": "nIf-5Y4zm3ND"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data scaling, extraction and feature selection\n",
        "In this section we will explore multiple scaling methods, determine if we will use PCA and define the treshold and we will explore mutliple feature selection methods. After this section we will have a dictionary containing the x_train data for all combinations of the chosen scaling, extraction (PCA) and selection methods."
      ],
      "metadata": {
        "id": "WIucopYVnYBI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "There are multiple ways to scale features by standarization and normalization."
      ],
      "metadata": {
        "id": "qYnQ0Ml9oA1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # 1. Standardization of the data\n",
        "# scaler_standard = StandardScaler()\n",
        "# x_scaled_standard = scaler_standard.fit_transform(x_train_imp) \n",
        "\n",
        "# # # 2. Min-max scaling\n",
        "# # scaler_minmax = MinMaxScaler()\n",
        "# # x_scaled_minmax = scaler_minmax.fit_transform(x_train_imp)\n",
        "\n",
        "# # 3. Robust scaling --> Robust is removed due to a to extensive loading time during the training of SVC\n",
        "# scaler_robust= RobustScaler()\n",
        "# x_scaled_robust = scaler_robust.fit_transform(x_train_imp)\n",
        "\n",
        "# x_scaled = {\"standard\":x_scaled_standard, \"robust\":x_scaled_robust}"
      ],
      "metadata": {
        "id": "7lPiglbIne_l"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going to perform PCA and determine the treshold."
      ],
      "metadata": {
        "id": "J9t4kREtoB0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # PCA\n",
        "# plt.close('all') #before plotting, close all old windows\n",
        "# x_scaled_pca = {} #create dictionary to add all combinations of scaling combined with PCA to\n",
        "\n",
        "# for key, x in x_scaled.items():\n",
        "    \n",
        "#     # applying pca\n",
        "#     pca_setting = PCA(n_components = 0.95) # threshold at 95% method\n",
        "#     x_pca = pca_setting.fit_transform(x)\n",
        "#     x_scaled_pca[f\"x_{key}_PCA\"] = x_pca\n",
        "\n",
        "#     # plot to show treshold choice\n",
        "#     pca_figure = PCA().fit(x)\n",
        "#     plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "#     fig, ax = plt.subplots()\n",
        "#     xi = np.arange(1, 662, step=1)\n",
        "#     y = np.cumsum(pca_figure.explained_variance_ratio_)\n",
        "\n",
        "#     plt.ylim(0.0,1.1)\n",
        "#     plt.plot(xi, y, marker='o', linestyle='--', color='b')\n",
        "\n",
        "#     plt.xlabel('Number of Components')\n",
        "#     plt.xticks(np.arange(0, 661, step=50)) #change from 0-based array index to 1-based human-readable label\n",
        "#     plt.ylabel('Cumulative variance (%)')\n",
        "#     plt.title(f'The number of components needed to explain variance for scaling method {key}')\n",
        "\n",
        "#     plt.axhline(y=0.95, color='r', linestyle='-')\n",
        "#     plt.text(0.5, 0.9, '95% cut-off threshold', color = 'red', fontsize=16)\n",
        "\n",
        "#     plt.axhline(y=0.85, color='r', linestyle='-')\n",
        "#     plt.text(0.5, 0.8, '85% cut-off threshold', color = 'red', fontsize=16)\n",
        "\n",
        "#     plt.axhline(y=0.75, color='r', linestyle='-')\n",
        "#     plt.text(0.5, 0.7, '75% cut-off threshold', color = 'red', fontsize=16)\n",
        "\n",
        "#     ax.grid(axis='x')\n",
        "#     plt.show()"
      ],
      "metadata": {
        "id": "nVCz7FgOnglf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasty we will look at feature selection. "
      ],
      "metadata": {
        "id": "4Cnit418Lpz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Feature selection with and without PCA\n",
        "# x_scaled_selected = {} #create dictionary to add all combinations of scaling with or without PCA combined with feature selection\n",
        "\n",
        "# selector = SelectKBest(f_classif)\n",
        "# k = 10\n",
        "\n",
        "# #Including PCA\n",
        "# for key, x in x_scaled_pca.items():\n",
        "#     selector.k = k\n",
        "#     x_ffs = selector.fit_transform(x, y_train)  # ffs = f-test feature selection\n",
        "#     x_scaled_selected[f\"{key}_ffs\"] =  x_ffs\n",
        "\n",
        "# #Excluding PCA\n",
        "# for key, x in x_scaled.items():\n",
        "#     selector.k = k\n",
        "#     x_ffs = selector.fit_transform(x, y_train)  # ffs = f-test feature selection\n",
        "#     x_scaled_selected[f\"x_{key}_ffs\"] =  x_ffs"
      ],
      "metadata": {
        "id": "JcMcEILqLpeU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we finish with a dictionary x_scaled_selected that inclused all combinations between feature scaling, feature extraction (PCA) and feature selection."
      ],
      "metadata": {
        "id": "mXpSLiVhLy0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Classifiers: hyperparameter tuning and fitting\n",
        "In this section we will build a neural network and train our different clasiffiers (standard and the neural network) and compare their functionality in terms of F1 and average precision. We will perform this for all possible combinations of scaling, selection and classification possibilities. This is also the section were we will tune our parameters."
      ],
      "metadata": {
        "id": "UXm9Ps4sm-v-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we are going to set the pipelines and parameters we are going to tune and write the function for tuning. "
      ],
      "metadata": {
        "id": "jWntzXM0oGlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a dictionary of all proposed pipelines\n",
        "k = 10\n",
        "rs = None #Set random state, 42 only added in the last run\n",
        "nc = 0.95 #Set n_components for PCA\n",
        "\n",
        "models_all_combinations = {\"standard_pca_ffs\":{\"KNeighborsClassifier\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()), (\"scaler\", StandardScaler()), (\"pca\", PCA()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', KNeighborsClassifier())]),\n",
        "          \"RandomForestClassifier\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", StandardScaler()), (\"pca\", PCA()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', RandomForestClassifier())]),\n",
        "          \"SVC\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", StandardScaler()), (\"pca\", PCA()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c',SVC())]),\n",
        "          \"MLP\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", StandardScaler()), (\"pca\", PCA()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', MLPClassifier())])},\n",
        "          \n",
        "          \"robust_pca_ffs\":{\"KNeighborsClassifier\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", RobustScaler()), (\"pca\", PCA()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', KNeighborsClassifier())]), \n",
        "          \"RandomForestClassifier\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", StandardScaler()), (\"pca\", PCA()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', RandomForestClassifier())]),\n",
        "          \"SVC\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", StandardScaler()), (\"pca\", PCA()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c',SVC())]),\n",
        "          \"MLP\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", StandardScaler()), (\"pca\", PCA()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', MLPClassifier())])},\n",
        "          \n",
        "          \"standard_ffs\":{\"KNeighborsClassifier\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", StandardScaler()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', KNeighborsClassifier())]),\n",
        "          \"RandomForestClassifier\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", StandardScaler()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', RandomForestClassifier())]),\n",
        "          \"SVC\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", StandardScaler()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c',SVC())]),\n",
        "          \"MLP\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", StandardScaler()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', MLPClassifier())])},\n",
        "          \n",
        "          \"robust_ffs\":{\"KNeighborsClassifier\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", RobustScaler()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', KNeighborsClassifier())]), \n",
        "          \"RandomForestClassifier\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", StandardScaler()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', RandomForestClassifier())]),\n",
        "          \"SVC\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", StandardScaler()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c',SVC())]),\n",
        "          \"MLP\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", StandardScaler()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', MLPClassifier())])}\n",
        "          }\n",
        "\n",
        "# Creating a dictionary of all parameters\n",
        "model_parameters = {\"KNeighborsClassifier\":{\"c__n_neighbors\":[x for x in range(1, 300)], \"c__weights\": [\"distance\", \"uniform\"]},\n",
        "                    \"RandomForestClassifier\":{\"c__n_estimators\":[x for x in range(2,100)], \"c__max_depth\":[x for x in range (1,100)], \"c__min_samples_leaf\": [x for x in range (50, 300)], \"c__random_state\" : [rs]},\n",
        "                    \"SVC\":{\"c__C\": [x for x in np.arange(0.0001, 50, 0.0001)], \"c__kernel\": [\"rbf\", \"poly\"], \"c__degree\":[x for x in range (2,4)], \"c__random_state\" : [rs]},\n",
        "                    \"MLP\": {\"c__hidden_layer_sizes\": [10,610,100], \"c__activation\": [\"relu\", \"logistic\", \"tanh\"], \"c__solver\": [\"sgd\", \"adam\", \"lbgfs\"], \n",
        "                            \"c__learning_rate\": [\"constant\", \"adaptive\"], \"c__max_iter\":[100], \"c__random_state\" : [rs]}\n",
        "                    }  \n",
        "\n",
        "param_with_PCA = {\"imp0__missing_values\": [0], \"pca__n_components\" : [nc], \"pca__random_state\": [rs], \"selector__k\": [k], \"sampling__sampling_strategy\": [\"minority\"], \"sampling__random_state\":[rs]}  \n",
        "param_without_PCA = {\"imp0__missing_values\": [0], \"pca__n_components\" : [nc], \"pca__random_state\": [rs], \"selector__k\": [k]}      \n",
        "\n"
      ],
      "metadata": {
        "id": "rB8WQrT8nBlD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going to write the functions to execute the Randomized search, return the needed results for the validation plots and plot the loss curves for the MLP classifier"
      ],
      "metadata": {
        "id": "aEPf-eqN5Nsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean up results and create dataframe with wanted results\n",
        "def get_dataframe_from_results(results, scorer):\n",
        "  \"\"\" This function makes sure that we can clean up the results that we get from the grid search and leaves is with the necessary results\"\"\"\n",
        "  df_to_concat = [pd.DataFrame(results[\"params\"])]\n",
        "\n",
        "  for key in scorer:\n",
        "    df_to_concat.append(pd.DataFrame(results[f\"mean_train_{key}\"], columns=[f\"mean_train_{key}\"]))\n",
        "    df_to_concat.append(pd.DataFrame(results[f\"mean_test_{key}\"], columns=[f\"mean_test_{key}\"]))\n",
        "    df_to_concat.append(pd.DataFrame(results[f\"std_train_{key}\"], columns=[f\"std_train_{key}\"]))\n",
        "    df_to_concat.append(pd.DataFrame(results[f\"std_test_{key}\"], columns=[f\"std_test_{key}\"]))\n",
        "\n",
        "  return pd.concat(df_to_concat, axis=1)\n",
        "\n",
        "# Function to do a grid search on each model using the parameters in the model_parameter dictionary\n",
        "def parametertuning(CombinationName, Models, ModelParameters, n_splits, n_iter, random_state, scorer, refit, X, Y):\n",
        "  \"\"\" This function performs the parameter tuning by doing a gridsearch with multiple parameters per model it returns the results that are cleaned up and only show the necessary\"\"\"\n",
        "  results = {} #create a dictionary to save results\n",
        "\n",
        "  for model_name, parameters in ModelParameters.items():\n",
        "    print(f\"Randomized searching {model_name}\")\n",
        "    model = Models[model_name]  # Find corresponding model in models dict\n",
        "\n",
        "    # Add constant parameters to the \n",
        "    if 'pca' in CombinationName:\n",
        "      params_complete = {**param_with_PCA, **parameters}\n",
        "    else:\n",
        "      params_complete = {**param_without_PCA, **parameters}\n",
        "\n",
        "    # Create the Cross vallidation object for inner and outer cross validation\n",
        "    cv_inner = KFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
        "    cv_outer = KFold(n_splits=n_splits, random_state=random_state, shuffle=True)   \n",
        "    \n",
        "    # Perform a grid search for the best parameters for the model using the test data and KFold\n",
        "    randomized_search = RandomizedSearchCV(model, params_complete, cv=cv_inner, n_iter=n_iter, n_jobs=-1, verbose=False, scoring=scorer, refit=refit, return_train_score=True, random_state = random_state) \n",
        "    randomized_search.fit(X, Y)\n",
        "\n",
        "    # Results of grid search\n",
        "    best_estimator = randomized_search.best_estimator_ # Returns the model with the best parameters filled in based on refit scoring metric\n",
        "    best_params = randomized_search.best_params_ \n",
        "    best_score = randomized_search.best_score_\n",
        "    randomized_search_results = get_dataframe_from_results(randomized_search.cv_results_, scorer) # Extracts the necessary results \n",
        "\n",
        "    #Compute the outer cross validation scores to compare models (also the accuracy to see if we still are above the 85% in the assignment)\n",
        "    a_outer = np.mean(cross_val_score(best_estimator, X, Y, cv=cv_outer)) \n",
        "    f1_outer = np.mean(cross_val_score(best_estimator, X, Y, cv=cv_outer, scoring = scorer[\"F1\"])) \n",
        "    AUC_outer = np.mean(cross_val_score(best_estimator, X, Y, cv=cv_outer, scoring = scorer[\"AUC\"])) \n",
        "\n",
        "    if model_name is \"MLP\":\n",
        "      plt.plot(Models[CombinationName][\"MLP\"].loss_curve_)\n",
        "      plt.title(f\"MLP loss curve for {CombinationName}\")\n",
        "      plt.xlabel('Epochs')\n",
        "      plt.ylabel('Loss')\n",
        "\n",
        "    # outer_scores = cross_validate(randomized_search, X, Y, cv=cv_outer, scoring = scorer, return_train_score = True, return_estimator = True) -- NOG VERWIJDEREN\n",
        "\n",
        "    # Add the results to the dictionary\n",
        "    results[model_name] = {\"BestEstimator\": best_estimator, \"BestParams\":best_params, \"BestScore\":best_score, \"Accuracy_outer\":a_outer, \"F1_outer\":f1_outer, \"AUC_outer\":AUC_outer, \"GSresults\": randomized_search_results}\n",
        "\n",
        "  \n",
        "  return results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zNkslru5CxC",
        "outputId": "54cf746d-a9f6-42e7-c3ea-c726868f7dcc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:48: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:48: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<ipython-input-14-3818ac33a0ac>:48: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if model_name is \"MLP\":\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going write the function to plot the validation curves to estimate if we are giving the randomized search the correct ranges."
      ],
      "metadata": {
        "id": "7Fo3B9GtoHfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions to create validation plots\n",
        "\n",
        "#---------------Plot for KNeighborsClassifier---------------\n",
        "def plot_knn(results, scoring, combination_name, ModelParameters):\n",
        "  knn_df = results[\"KNeighborsClassifier\"][\"GSresults\"]\n",
        "  uni_knn_df = knn_df[knn_df[\"c__weights\"] == \"uniform\"]\n",
        "  dist_knn_df = knn_df[knn_df[\"c__weights\"] == \"distance\"]\n",
        "\n",
        "  fig, ax = plt.subplots(2,2) #create subplot\n",
        "  fig.suptitle(f\"KNN validation plots for {combination_name}\", fontsize=12)\n",
        "\n",
        "  # For uniform weight\n",
        "  grouped_n = uni_knn_df.groupby(by=[\"c__n_neighbors\"]).mean()\n",
        "  param_list = grouped_n.index.to_numpy()  # Pakt de parameters die op de x as gezet worden\n",
        "\n",
        "  for score, i in zip(scoring, range(0,2)): \n",
        "    ax[i,0].plot(param_list, grouped_n[f\"mean_train_{score}\"], label=\"Train\")  # plot the train F1 score\n",
        "    ax[i,0].plot(param_list, grouped_n[f\"mean_test_{score}\"], label=f\"Test\")  # plot the test F1 score\n",
        "    ax[i,0].set_title(f\"Uniform {score}\")\n",
        "    ax[i,0].set_xlabel(f\"n_neighbors\")\n",
        "    ax[i,0].set_ylabel(f\"{score}\")\n",
        "    ax[i,0].legend(loc='lower right')\n",
        "\n",
        "  # For distance weight\n",
        "  grouped_n = dist_knn_df.groupby(by=[\"c__n_neighbors\"]).mean()\n",
        "  param_list = grouped_n.index.to_numpy()  # Pakt de parameters die op de x as gezet worden\n",
        "\n",
        "  for score, i in zip(scoring, range(0,2)): \n",
        "    ax[i,1].plot(param_list, grouped_n[f\"mean_train_{score}\"], label=\"Train\")  # plot the train F1 score\n",
        "    ax[i,1].plot(param_list, grouped_n[f\"mean_test_{score}\"], label=\"Test\")  # plot the test F1 score\n",
        "    ax[i,1].set_title(f\"Distance {score}\")\n",
        "    ax[i,1].set_xlabel(f\"n_neighbors\")\n",
        "    ax[i,1].set_ylabel(f\"{score}\")\n",
        "    ax[i,1].legend(loc='lower right')\n",
        "\n",
        "  \n",
        "  for ax in fig.get_axes():\n",
        "    ax.label_outer()\n",
        "  \n",
        "#---------------Plot for SVC Classifier--------------- \n",
        "def plot_svc(results, scoring, combination_name, ModelParameters):\n",
        "  svc_df = results[\"SVC\"][\"GSresults\"]\n",
        "  rbf_svc_df = svc_df[svc_df[\"c__kernel\"]==\"rbf\"]\n",
        "  poly_svc_df = svc_df[svc_df[\"c__kernel\"]==\"poly\"]\n",
        "\n",
        "  fig, ax = plt.subplots(2,3) #create subplot\n",
        "  fig.suptitle(f\"SVC validation plots for {combination_name}\", fontsize=12)\n",
        "\n",
        "  # For RBF kernel\n",
        "  grouped_C = rbf_svc_df.groupby(by=[\"c__C\"]).mean()\n",
        "  param_list = grouped_C.index.to_numpy()  # Pakt de parameters die op de x as gezet worden\n",
        "\n",
        "  for score, i in zip(scoring, range(0,2)): \n",
        "    ax[i,0].plot(param_list, grouped_C[f\"mean_train_{score}\"], label=\"Train\")  # plot the train F1 score\n",
        "    ax[i,0].plot(param_list, grouped_C[f\"mean_test_{score}\"], label=\"Test\")  # plot the test F1 score\n",
        "    ax[i,0].set_title(f\"RBF {score}\")\n",
        "    ax[i,0].set_xlabel(f\"C\")\n",
        "    ax[i,0].set_ylabel(f\"{score}\")\n",
        "    ax[i,0].legend(loc='lower right')\n",
        "    \n",
        "  #For Poly kernels \n",
        "  for degree, i in zip(range(2,4), range(1,3)):\n",
        "    degree_svc_df = poly_svc_df[poly_svc_df[\"c__degree\"]==degree]\n",
        "    grouped_C = degree_svc_df.groupby(by=[\"c__C\"]).mean()\n",
        "    param_list = grouped_C.index.to_numpy()  # Pakt de parameters die op de x as gezet worden\n",
        "\n",
        "    for score, j in zip(scoring, range(0,2)): \n",
        "      ax[j,i].plot(param_list, grouped_C[f\"mean_train_{score}\"], label=\"Train\")  # plot the train F1 score\n",
        "      ax[j,i].plot(param_list, grouped_C[f\"mean_test_{score}\"], label=\"Test\")  # plot the test F1 score\n",
        "      ax[j,i].set_title(f\"Poly degree {degree} {score}\")\n",
        "      ax[j,i].set_xlabel(f\"C\")\n",
        "      ax[j,i].set_ylabel(f\"{score}\")\n",
        "      ax[j,i].legend(loc='lower right')\n",
        "\n",
        "  for ax in fig.get_axes():\n",
        "    ax.label_outer()\n",
        "       \n",
        "# ---------------Plot for RandomForestClassifier---------------\n",
        "def plot_randomforest(results, scoring, combination_name, ModelParameters):\n",
        "    forest_df = results[\"RandomForestClassifier\"][\"GSresults\"]\n",
        "\n",
        "    fig, ax = plt.subplots(2,3) #create subplot\n",
        "    fig.suptitle(f\"Decision tree validation plots for {combination_name}\", fontsize=12)\n",
        "    \n",
        "    for param, j in zip(ModelParameters[\"RandomForestClassifier\"], range(0,3)):\n",
        "      grouped = forest_df.groupby(by=[param]).mean()\n",
        "      param_list = grouped.index.to_numpy()  # Pakt de parameters die op de x as gezet worden\n",
        "      \n",
        "      for score, i in zip(scoring, range(0,2)): \n",
        "        ax[i,j].plot(param_list, grouped[f\"mean_train_{score}\"], label=\"Train\")  # plot the train F1 score\n",
        "        ax[i,j].plot(param_list, grouped[f\"mean_test_{score}\"], label=\"Test\")  # plot the test F1 score\n",
        "        ax[i,j].set_title(f\"{param.replace('c__', '')}\")\n",
        "        ax[i,j].set_xlabel(f\"{param.replace('c__', '')}\")\n",
        "        ax[i,j].set_ylabel(f\"{score}\")\n",
        "        ax[i,j].legend(loc='lower right')\n",
        "    \n",
        "    for ax in fig.get_axes():\n",
        "      ax.label_outer()"
      ],
      "metadata": {
        "id": "YS-1NeZunEfi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beneath, we write the function to display the best scoring results for each model."
      ],
      "metadata": {
        "id": "DjUe1_wAoJpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot tabel with scoring metric and optimal paramater combination per scalor and selection\n",
        "def create_table_of_results(results_all_combinations, combination_name):\n",
        "  model_names = [] \n",
        "  best_params = []\n",
        "  best_scores = []\n",
        "\n",
        "  # F1_scores = []\n",
        "  # AUC_scores = []\n",
        "  Mean_Accuracy = []\n",
        "  Mean_F1 = []\n",
        "  Mean_AUC = []\n",
        "\n",
        "  results = results_all_combinations[combination_name]\n",
        "\n",
        "  for model_name in results:\n",
        "\n",
        "    # Create column with model names\n",
        "    model_names.append(model_name)\n",
        "\n",
        "    # Create column with best params\n",
        "    best_param = results[model_name][\"BestParams\"]\n",
        "    best_params.append(best_param)\n",
        "\n",
        "    # Create column with best inner score\n",
        "    best_score = results[model_name][\"BestScore\"]\n",
        "    best_scores.append(best_score)\n",
        "\n",
        "    # # Create column with F1 scores of inner cross validation\n",
        "    # F1 = results[model_name][\"GSresults\"][\"mean_test_F1\"].max()\n",
        "    # F1_scores.append(F1)\n",
        "\n",
        "    # # Create column with Average precision scores of inner cross validation\n",
        "    # AUC = results[model_name][\"GSresults\"][\"mean_test_AUC\"].max()\n",
        "    # AUC_scores.append(AUC)\n",
        "    \n",
        "    # Create column with accuracy scores of outer cross validation of best estimator\n",
        "    mean_accuracy = results[model_name][\"Accuracy_outer\"]\n",
        "    Mean_Accuracy.append(mean_accuracy)\n",
        "\n",
        "    # Create column with accuracy scores of outer cross validation of best estimator\n",
        "    mean_f1 = results[model_name][\"F1_outer\"]\n",
        "    Mean_F1.append(mean_f1)\n",
        "    \n",
        "    # Create column with accuracy scores of outer cross validation of best estimator\n",
        "    mean_AUC = results[model_name][\"AUC_outer\"]\n",
        "    Mean_AUC.append(mean_AUC)\n",
        "\n",
        "  print(f\"Table of results for combination {combination_name}\")\n",
        "  display(pd.DataFrame({\"ModelName\":model_names, \"BestParams\":best_params, \"Best inner F1 score\": best_scores, \"Outer accuracy of BE\":Mean_Accuracy, \"Outer F1 of BE\":Mean_F1, \"Outer AUC of BE\":Mean_AUC}).style.hide(axis='index'))\n",
        "  print()"
      ],
      "metadata": {
        "id": "a3_dsVkEoKto"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we will call all functions written to tune all the parameters and plot the results for each combination of scaling, PCA and selection."
      ],
      "metadata": {
        "id": "IA5YZ10iM5OG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform training\n",
        "results_all_combinations = {}\n",
        "\n",
        "for combination_name, models in models_all_combinations.items():\n",
        "\n",
        "  #Create scorers\n",
        "  f1_scorer = make_scorer(f1_score, labels = None, average = 'binary', pos_label = 1) \n",
        "  scoring = {\"F1\":f1_scorer, \"AUC\":\"roc_auc\"} #Determine what type of scoring you want to use\n",
        "  \n",
        "  #Ignorme warnings from MLP\n",
        "  import warnings\n",
        "  warnings.filterwarnings('ignore') \n",
        "\n",
        "  # Perform randomized search\n",
        "  results = parametertuning(combination_name, models, model_parameters, 5, 50, None, scoring, \"F1\", x_train, y_train)\n",
        "  results_all_combinations[combination_name] = results"
      ],
      "metadata": {
        "id": "isqnuYlhM6Oz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "15deae15-8739-4f77-c678-f1e02edc6399"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Randomized searching KNeighborsClassifier\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.9/dist-packages/joblib/externals/loky/process_executor.py\", line 428, in _process_worker\n    r = call_item()\n  File \"/usr/local/lib/python3.9/dist-packages/joblib/externals/loky/process_executor.py\", line 275, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n  File \"/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n  File \"/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/usr/local/lib/python3.9/dist-packages/sklearn/utils/parallel.py\", line 123, in __call__\n    return self.function(*args, **kwargs)\n  File \"/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\", line 674, in _fit_and_score\n    estimator = estimator.set_params(**cloned_parameters)\n  File \"/usr/local/lib/python3.9/dist-packages/sklearn/pipeline.py\", line 211, in set_params\n    self._set_params(\"steps\", **kwargs)\n  File \"/usr/local/lib/python3.9/dist-packages/sklearn/utils/metaestimators.py\", line 70, in _set_params\n    super().set_params(**params)\n  File \"/usr/local/lib/python3.9/dist-packages/sklearn/base.py\", line 205, in set_params\n    raise ValueError(\nValueError: Invalid parameter 'PCA' for estimator Pipeline(steps=[('impNaN', SimpleImputer()), ('imp0', SimpleImputer()),\n                ('scaler', StandardScaler()), ('pca', PCA()),\n                ('selector', SelectKBest()), ('sampling', SMOTE()),\n                ('c', KNeighborsClassifier())]). Valid parameters are: ['memory', 'steps', 'verbose'].\n\"\"\"",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-11d778a4a944>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m# Perform randomized search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparametertuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombination_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"F1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0mresults_all_combinations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcombination_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-3818ac33a0ac>\u001b[0m in \u001b[0;36mparametertuning\u001b[0;34m(CombinationName, Models, ModelParameters, n_splits, n_iter, random_state, scorer, refit, X, Y)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Perform a grid search for the best parameters for the model using the test data and KFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mrandomized_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_complete\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv_inner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrefit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mrandomized_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Results of grid search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1769\u001b[0m             ParameterSampler(\n\u001b[1;32m   1770\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    565\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    566\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    444\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid parameter 'PCA' for estimator Pipeline(steps=[('impNaN', SimpleImputer()), ('imp0', SimpleImputer()),\n                ('scaler', StandardScaler()), ('pca', PCA()),\n                ('selector', SelectKBest()), ('sampling', SMOTE()),\n                ('c', KNeighborsClassifier())]). Valid parameters are: ['memory', 'steps', 'verbose']."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create visualization of results \n",
        "plt.close('all') #make sure you class all windows before plotting to prevent plotting in the same figure\n",
        "\n",
        "for combination_name in models_all_combinations:\n",
        "\n",
        "  #Create all validation plots to show the relation between the scoring type and parameters for every combination\n",
        "  plot_knn(results_all_combinations[combination_name], scoring, combination_name, model_parameters)\n",
        "  plot_svc(results_all_combinations[combination_name], scoring, combination_name, model_parameters)\n",
        "  plot_randomforest(results_all_combinations[combination_name], scoring, combination_name, model_parameters)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "srThEOW1NPv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create table with results for every combination\n",
        "for combination_name in models_all_combinations:\n",
        "  create_table_of_results(results_all_combinations, combination_name)"
      ],
      "metadata": {
        "id": "S3yr30PFNSgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. The final model\n",
        "After the previous steps we determined the best combination of scaling, PCA, feature selection and classification for our final model. Underneath we wrote the final pipeline that returns the mean accuracy of the model on the test data."
      ],
      "metadata": {
        "id": "sqnvS2MX2ozi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write final pipeline with chosen combinations\n",
        "def final_pipeline(x_train, y_train, results_all_combinations):\n",
        "    \"\"\"This function goes trough all the steps of the final model setup chosen in the previous section. Note: you need to run sections loading packages, 1, 2 and 4 in order to run this code\"\"\"\n",
        "    \n",
        "    # Determine steps of the pipeline\n",
        "    pipe = results_all_combinations[\"robust_ffs\"][\"SVC\"][\"BestEstimator\"]\n",
        "    print(f\"The steps of the final model look are {pipe}\\n\")\n",
        "\n",
        "    pipe_params = pipe.get_params()\n",
        "    print(pipe_params)\n",
        "\n",
        "\n",
        "    # Fit the pipeline on the training data \n",
        "    pipe.fit(x_train, y_train)\n",
        "\n",
        "    # Compute the mean accuracy of the model on the test data\n",
        "    y_pred= pipe.predict(x_test)\n",
        "    score_f1 = f1_score(y_test, y_pred)\n",
        "    score_AUC = roc_auc_score(y_test, y_pred)\n",
        "    score_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    return score_f1, score_AUC, score_accuracy\n",
        "\n",
        "#Execute pipeline to get the mean accuracy of the final model on the test data\n",
        "score_f1, score_AUC, score_accuracy = final_pipeline(x_train, y_train, results_all_combinations)\n",
        "print(f\"The scores of the final model are: f1 = {score_f1}, AUC = {score_AUC} and accuracy = {score_accuracy}\")\n"
      ],
      "metadata": {
        "id": "QAaZY7vY2vJg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
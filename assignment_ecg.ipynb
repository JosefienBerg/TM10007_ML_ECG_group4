{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SXpaKwwGe5x"
      },
      "source": [
        "# TM10007 Group 4 -- ECG Data\n",
        "Lara de Bats (5022037), Josefien van den Berg (4663381), Merel Goossens (4856902), Amber Liqui Lung (4464168)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment we wil test certain classifiers in order to create a classifier model to distinguish between normal and abnormal ECG's. The code is divided in different sections that are explained in the corresponding method sections in our report."
      ],
      "metadata": {
        "id": "u0jPpiE5pyEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading packages\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "import numpy as np\n",
        "import seaborn\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab as pl\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.model_selection import KFold, GridSearchCV, RandomizedSearchCV, train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import f1_score, make_scorer, accuracy_score, roc_auc_score\n",
        "# from sklearn.pipeline import Pipeline\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn import set_config\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "pd.set_option('display.max_rows', 30)"
      ],
      "metadata": {
        "id": "f-5Aw7sQpsDL"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EG78VSHfhNi"
      },
      "source": [
        "## 1. Data loading, initial split, exploring and cleaning\n",
        "\n",
        "Below we load the ECG data, convert it to a dataframe an clean the data.First we are going to start with loading the CS file into a dataframe. Then we are going to split the dataset into test data and training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "CiDn2Sk-VWqE",
        "outputId": "08bfa71b-d053-4c11-c4cb-e01f1540b0a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'TM10007_ML_ECG_group4' already exists and is not an empty directory.\n",
            "The number of samples: 827\n",
            "The number of columns: 9001\n"
          ]
        }
      ],
      "source": [
        "# Load CSV file into dataframe\n",
        "!git clone https://github.com/JosefienBerg/TM10007_ML_ECG_group4.git\n",
        "\n",
        "with zipfile.ZipFile('/content/TM10007_ML_ECG_group4/ecg/ecg_data.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/TM10007_ML_ECG_group4/ecg')\n",
        "\n",
        "df = pd.read_csv('/content/TM10007_ML_ECG_group4/ecg/ecg_data.csv', index_col=0)\n",
        "\n",
        "print(f'The number of samples: {len(df.index)}')\n",
        "print(f'The number of columns: {len(df.columns)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Underneath we explore our dataset"
      ],
      "metadata": {
        "id": "OaN6z_VynlYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many patients have a normal or abnormal ECG?\n",
        "count0= (df['label'] == 0).sum()\n",
        "count1= (df['label'] == 1).sum()\n",
        "print(f'There are {count0} patients with label 0')\n",
        "print(f'There are {count1} patients with label 1')\n",
        "# Since the majority of patients has no abnormalities we can conclude that label 0 is normal and label 1 is abnormal "
      ],
      "metadata": {
        "id": "6uzs9dYjnrHt",
        "outputId": "c091dffc-5e32-4c99-b525-9469261676a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 681 patients with label 0\n",
            "There are 146 patients with label 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will perform our initial split"
      ],
      "metadata": {
        "id": "ORtoTNM7LF54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the whole data set in a training and test set\n",
        "features = df.loc[:, df.columns !=\"label\"].to_numpy()\n",
        "labels = df[\"label\"].to_numpy()\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split (features, labels, train_size = 0.8, random_state = 42) "
      ],
      "metadata": {
        "id": "7WHZbHF0LKCk"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets explore x_train"
      ],
      "metadata": {
        "id": "jhZt3EF9Wf3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at the size\n",
        "print(f\"The training set has {x_train.shape[0]} samples and {x_train.shape[1]} features with {y_train.shape[0]} labels\")\n",
        "print(f\"The test has {x_test.shape[0]} samples and {x_test.shape[1]} features with {y_test.shape[0]} labels\")\n",
        "\n",
        "# Look at the number of label 1 and label 0 in trainingset\n",
        "count0= (y_train == 0).sum()\n",
        "count1= (y_train == 1).sum()\n",
        "print(f'There are {count0} patients with label 0')\n",
        "print(f'There are {count1} patients with label 1')\n",
        "\n",
        "# Look at the distribution\n",
        "df = pd.DataFrame(x_train)\n",
        "display(df.describe())\n",
        "\n",
        "# Look at the type of data\n",
        "print(x_train.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "g3RxneYdWh2Y",
        "outputId": "9e91e3db-2a95-4499-dee2-63db454519e2"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The training set has 661 samples and 9000 features with 661 labels\n",
            "The test has 166 samples and 9000 features with 166 labels\n",
            "There are 543 patients with label 0\n",
            "There are 118 patients with label 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "              0            1           2           3            4     \\\n",
              "count   661.000000   661.000000  661.000000  661.000000   661.000000   \n",
              "mean    473.545787    68.219735   40.107831   30.223831    32.912681   \n",
              "std    1090.597625   183.150669   82.932772   56.989313    58.226109   \n",
              "min       0.031717     0.338026    0.374062    0.392388     0.442473   \n",
              "25%       4.048258     6.697330    7.502202    7.973962     9.699800   \n",
              "50%      11.301568    14.319638   16.019461   15.585303    18.526251   \n",
              "75%     414.437620    47.007877   36.407946   29.570521    34.987714   \n",
              "max    9510.877805  2022.415412  963.283307  776.631173  1016.707977   \n",
              "\n",
              "             5            6           7           8           9     ...  \\\n",
              "count  661.000000   661.000000  661.000000  661.000000  661.000000  ...   \n",
              "mean    42.555519    54.601058   38.398261   27.756282   22.688850  ...   \n",
              "std     46.396159    61.963314   52.542619   41.189322   36.309774  ...   \n",
              "min      0.468230     0.291254    0.181994    0.168997    0.732216  ...   \n",
              "25%     13.513050    17.151425   11.714197    8.290047    7.268737  ...   \n",
              "50%     26.475948    39.914146   24.261486   15.910253   13.764587  ...   \n",
              "75%     53.298597    77.536913   48.212765   30.232692   25.157698  ...   \n",
              "max    443.762983  1051.557893  775.236262  483.852043  548.180243  ...   \n",
              "\n",
              "             8990        8991        8992        8993        8994        8995  \\\n",
              "count  661.000000  661.000000  661.000000  661.000000  661.000000  661.000000   \n",
              "mean     0.257628    0.256989    0.260101    0.261333    0.258008    0.259235   \n",
              "std      0.466041    0.492337    0.453205    0.485969    0.523492    0.561868   \n",
              "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
              "25%      0.083260    0.077778    0.085630    0.083037    0.077072    0.084123   \n",
              "50%      0.142454    0.140854    0.148017    0.139903    0.139145    0.132906   \n",
              "75%      0.254538    0.242179    0.261747    0.250637    0.241596    0.242516   \n",
              "max      6.808234    7.650954    8.154244    8.203337    8.772557    8.889082   \n",
              "\n",
              "             8996        8997        8998        8999  \n",
              "count  661.000000  661.000000  661.000000  661.000000  \n",
              "mean     0.258205    0.251576    0.259956    0.258827  \n",
              "std      0.473904    0.472443    0.533672    0.534501  \n",
              "min      0.000000    0.000000    0.000000    0.000000  \n",
              "25%      0.084571    0.079911    0.074682    0.078336  \n",
              "50%      0.141514    0.137632    0.141386    0.132081  \n",
              "75%      0.260533    0.253220    0.249928    0.244466  \n",
              "max      9.048367    9.013666    9.058323    9.035147  \n",
              "\n",
              "[8 rows x 9000 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bf6eb03d-24db-47dc-8553-886816899cae\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>8990</th>\n",
              "      <th>8991</th>\n",
              "      <th>8992</th>\n",
              "      <th>8993</th>\n",
              "      <th>8994</th>\n",
              "      <th>8995</th>\n",
              "      <th>8996</th>\n",
              "      <th>8997</th>\n",
              "      <th>8998</th>\n",
              "      <th>8999</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>661.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>473.545787</td>\n",
              "      <td>68.219735</td>\n",
              "      <td>40.107831</td>\n",
              "      <td>30.223831</td>\n",
              "      <td>32.912681</td>\n",
              "      <td>42.555519</td>\n",
              "      <td>54.601058</td>\n",
              "      <td>38.398261</td>\n",
              "      <td>27.756282</td>\n",
              "      <td>22.688850</td>\n",
              "      <td>...</td>\n",
              "      <td>0.257628</td>\n",
              "      <td>0.256989</td>\n",
              "      <td>0.260101</td>\n",
              "      <td>0.261333</td>\n",
              "      <td>0.258008</td>\n",
              "      <td>0.259235</td>\n",
              "      <td>0.258205</td>\n",
              "      <td>0.251576</td>\n",
              "      <td>0.259956</td>\n",
              "      <td>0.258827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1090.597625</td>\n",
              "      <td>183.150669</td>\n",
              "      <td>82.932772</td>\n",
              "      <td>56.989313</td>\n",
              "      <td>58.226109</td>\n",
              "      <td>46.396159</td>\n",
              "      <td>61.963314</td>\n",
              "      <td>52.542619</td>\n",
              "      <td>41.189322</td>\n",
              "      <td>36.309774</td>\n",
              "      <td>...</td>\n",
              "      <td>0.466041</td>\n",
              "      <td>0.492337</td>\n",
              "      <td>0.453205</td>\n",
              "      <td>0.485969</td>\n",
              "      <td>0.523492</td>\n",
              "      <td>0.561868</td>\n",
              "      <td>0.473904</td>\n",
              "      <td>0.472443</td>\n",
              "      <td>0.533672</td>\n",
              "      <td>0.534501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.031717</td>\n",
              "      <td>0.338026</td>\n",
              "      <td>0.374062</td>\n",
              "      <td>0.392388</td>\n",
              "      <td>0.442473</td>\n",
              "      <td>0.468230</td>\n",
              "      <td>0.291254</td>\n",
              "      <td>0.181994</td>\n",
              "      <td>0.168997</td>\n",
              "      <td>0.732216</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>4.048258</td>\n",
              "      <td>6.697330</td>\n",
              "      <td>7.502202</td>\n",
              "      <td>7.973962</td>\n",
              "      <td>9.699800</td>\n",
              "      <td>13.513050</td>\n",
              "      <td>17.151425</td>\n",
              "      <td>11.714197</td>\n",
              "      <td>8.290047</td>\n",
              "      <td>7.268737</td>\n",
              "      <td>...</td>\n",
              "      <td>0.083260</td>\n",
              "      <td>0.077778</td>\n",
              "      <td>0.085630</td>\n",
              "      <td>0.083037</td>\n",
              "      <td>0.077072</td>\n",
              "      <td>0.084123</td>\n",
              "      <td>0.084571</td>\n",
              "      <td>0.079911</td>\n",
              "      <td>0.074682</td>\n",
              "      <td>0.078336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>11.301568</td>\n",
              "      <td>14.319638</td>\n",
              "      <td>16.019461</td>\n",
              "      <td>15.585303</td>\n",
              "      <td>18.526251</td>\n",
              "      <td>26.475948</td>\n",
              "      <td>39.914146</td>\n",
              "      <td>24.261486</td>\n",
              "      <td>15.910253</td>\n",
              "      <td>13.764587</td>\n",
              "      <td>...</td>\n",
              "      <td>0.142454</td>\n",
              "      <td>0.140854</td>\n",
              "      <td>0.148017</td>\n",
              "      <td>0.139903</td>\n",
              "      <td>0.139145</td>\n",
              "      <td>0.132906</td>\n",
              "      <td>0.141514</td>\n",
              "      <td>0.137632</td>\n",
              "      <td>0.141386</td>\n",
              "      <td>0.132081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>414.437620</td>\n",
              "      <td>47.007877</td>\n",
              "      <td>36.407946</td>\n",
              "      <td>29.570521</td>\n",
              "      <td>34.987714</td>\n",
              "      <td>53.298597</td>\n",
              "      <td>77.536913</td>\n",
              "      <td>48.212765</td>\n",
              "      <td>30.232692</td>\n",
              "      <td>25.157698</td>\n",
              "      <td>...</td>\n",
              "      <td>0.254538</td>\n",
              "      <td>0.242179</td>\n",
              "      <td>0.261747</td>\n",
              "      <td>0.250637</td>\n",
              "      <td>0.241596</td>\n",
              "      <td>0.242516</td>\n",
              "      <td>0.260533</td>\n",
              "      <td>0.253220</td>\n",
              "      <td>0.249928</td>\n",
              "      <td>0.244466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>9510.877805</td>\n",
              "      <td>2022.415412</td>\n",
              "      <td>963.283307</td>\n",
              "      <td>776.631173</td>\n",
              "      <td>1016.707977</td>\n",
              "      <td>443.762983</td>\n",
              "      <td>1051.557893</td>\n",
              "      <td>775.236262</td>\n",
              "      <td>483.852043</td>\n",
              "      <td>548.180243</td>\n",
              "      <td>...</td>\n",
              "      <td>6.808234</td>\n",
              "      <td>7.650954</td>\n",
              "      <td>8.154244</td>\n",
              "      <td>8.203337</td>\n",
              "      <td>8.772557</td>\n",
              "      <td>8.889082</td>\n",
              "      <td>9.048367</td>\n",
              "      <td>9.013666</td>\n",
              "      <td>9.058323</td>\n",
              "      <td>9.035147</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows Ã— 9000 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bf6eb03d-24db-47dc-8553-886816899cae')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bf6eb03d-24db-47dc-8553-886816899cae button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bf6eb03d-24db-47dc-8553-886816899cae');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a last step, we want do delelte entire rows and columns with only zeros or NaN/None values as they can be seen as missing data."
      ],
      "metadata": {
        "id": "We8ePYjT1Qid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Delete rows and columns with only zero's or NaN's\n",
        "def delete_missing_rows(x, y):\n",
        "  concat = np.c_[x, y]\n",
        "\n",
        "  for row in x:\n",
        "    index = np.where((row == 0).all) or np.where((row == np.nan).all)\n",
        "    np.delete(concat, index, 0)\n",
        "  \n",
        "  x =  concat[:,:-1]\n",
        "  y = concat[:,-1] \n",
        "  return x, y\n",
        "\n",
        "x_train, y_train = delete_missing_rows(x_train, y_train)"
      ],
      "metadata": {
        "id": "zuqOeZIQ1FSS"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data preprocessing\n",
        "In this section we explore the data preprocessing steps before implementing them in the pipelines in section 3. Here we manually create the pipelines in order to test if the functions work and to determine the prinicpal components for PCA. \n"
      ],
      "metadata": {
        "id": "hbyr7EH1fUE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1.1 Imputation**; \n",
        "Underneath we try out the functions for the imputation steps. Datasets with missing values that are filled with placeholders such as NaN and None, can cause problems when using estimators. Therefore you want to perform imputation in order to create usable datasets. "
      ],
      "metadata": {
        "id": "iiggNmNvmsOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #Replace nan's and zeros since the chance of a zero is very low due to only float types\n",
        "# def replace_missing_values(x): \n",
        "#   imp = SimpleImputer(missing_values = np.nan, strategy='mean', copy = False)\n",
        "#   imp.fit_transform(x, y=None) #removes entire columns if it contains only missing values\n",
        "\n",
        "#   imp = SimpleImputer(missing_values = 0, strategy='mean', copy = False)\n",
        "#   imp.fit_transform(x, y=None) #removes entire columns if it contains only missing values\n",
        "#   return x\n",
        "\n",
        "# # Call functions to perform imputation steps and rename x_train\n",
        "# x_train_imp = replace_missing_values(x_train)\n",
        "\n",
        "# count0= (x_train_imp == 0).sum() \n",
        "# countNaN = np.isnan(x_train_imp)[np.isnan(x_train_imp) == True].size\n",
        "# print(f\"There are {count0} zeros and {countNaN} NaN values left in the dataset\")"
      ],
      "metadata": {
        "id": "nIf-5Y4zm3ND"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1.2 Feature scaling**; Underneath we try the chosen scaling methods Robust and MinMax"
      ],
      "metadata": {
        "id": "qYnQ0Ml9oA1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # 1. Standardization of the data\n",
        "# scaler_standard = StandardScaler()\n",
        "# x_scaled_standard = scaler_standard.fit_transform(x_train_imp) \n",
        "\n",
        "# # # 2. Min-max scaling\n",
        "# # scaler_minmax = MinMaxScaler()\n",
        "# # x_scaled_minmax = scaler_minmax.fit_transform(x_train_imp)\n",
        "\n",
        "# # 3. Robust scaling --> Robust is removed due to a to extensive loading time during the training of SVC\n",
        "# scaler_robust= RobustScaler()\n",
        "# x_scaled_robust = scaler_robust.fit_transform(x_train_imp)\n",
        "\n",
        "# x_scaled = {\"standard\":x_scaled_standard, \"robust\":x_scaled_robust}"
      ],
      "metadata": {
        "id": "7lPiglbIne_l"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1.3 Feature Extraction**; Now we are going to investigate the n_components for the possible usage of PCA"
      ],
      "metadata": {
        "id": "J9t4kREtoB0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # PCA\n",
        "# plt.close('all') #before plotting, close all old windows\n",
        "# x_scaled_pca = {} #create dictionary to add all combinations of scaling combined with PCA to\n",
        "\n",
        "# for key, x in x_scaled.items():\n",
        "    \n",
        "#     # applying pca\n",
        "#     pca_setting = PCA(n_components = 0.95) # threshold at 95% method\n",
        "#     x_pca = pca_setting.fit_transform(x)\n",
        "#     x_scaled_pca[f\"x_{key}_PCA\"] = x_pca\n",
        "\n",
        "#     # plot to show treshold choice\n",
        "#     pca_figure = PCA().fit(x)\n",
        "#     plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "#     fig, ax = plt.subplots()\n",
        "#     xi = np.arange(1, 662, step=1)\n",
        "#     y = np.cumsum(pca_figure.explained_variance_ratio_)\n",
        "\n",
        "#     plt.ylim(0.0,1.1)\n",
        "#     plt.plot(xi, y, marker='o', linestyle='--', color='b')\n",
        "\n",
        "#     plt.xlabel('Number of Components')\n",
        "#     plt.xticks(np.arange(0, 661, step=50)) #change from 0-based array index to 1-based human-readable label\n",
        "#     plt.ylabel('Cumulative variance (%)')\n",
        "#     plt.title(f'The number of components needed to explain variance for scaling method {key}')\n",
        "\n",
        "#     plt.axhline(y=0.95, color='r', linestyle='-')\n",
        "#     plt.text(0.5, 0.9, '95% cut-off threshold', color = 'red', fontsize=16)\n",
        "\n",
        "#     plt.axhline(y=0.85, color='r', linestyle='-')\n",
        "#     plt.text(0.5, 0.8, '85% cut-off threshold', color = 'red', fontsize=16)\n",
        "\n",
        "#     plt.axhline(y=0.75, color='r', linestyle='-')\n",
        "#     plt.text(0.5, 0.7, '75% cut-off threshold', color = 'red', fontsize=16)\n",
        "\n",
        "#     ax.grid(axis='x')\n",
        "#     plt.show()"
      ],
      "metadata": {
        "id": "nVCz7FgOnglf"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1.4 Feature Selection**; For feature selection, the SelectKBest is used as explained in the method, this is as trandard function directly implemented in the pipelines in the next section."
      ],
      "metadata": {
        "id": "mXpSLiVhLy0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1.5 Creating pipelines**; this step is performed in the next section combined with directly adding the SMOTE and classifiers in the models_all_combinations dictionary."
      ],
      "metadata": {
        "id": "hmmzao3sp7mv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Classifiers: hyperparameter tuning and fitting\n",
        "In this section we will build a neural network and train our different clasiffiers (standard and the neural network) and compare their functionality in terms of F1 and AUC. The accuracy is also provided to see if we are still close to the 83% when randomly pikking. We will perform this for all possible combinations of scaling, selection and classification possibilities. This is also the section were we will tune our parameters."
      ],
      "metadata": {
        "id": "UXm9Ps4sm-v-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we are going to set the pipelines and parameters we are going to tune and write the function for tuning. "
      ],
      "metadata": {
        "id": "jWntzXM0oGlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a dictionary of all proposed pipelines\n",
        "k = 10\n",
        "rs = None #Set random state, 42 only added in the last run\n",
        "nc = 0.95 #Set n_components for PCA\n",
        "\n",
        "models_all_combinations = {\"minmax_pca_ffs\":{\"KNeighborsClassifier\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()), (\"scaler\", MinMaxScaler()), (\"pca\", PCA()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', KNeighborsClassifier())]),\n",
        "          \"RandomForestClassifier\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", MinMaxScaler()), (\"pca\", PCA()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', RandomForestClassifier())]),\n",
        "          \"SVC\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", MinMaxScaler()), (\"pca\", PCA()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c',SVC())]),\n",
        "          \"MLP\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", MinMaxScaler()), (\"pca\", PCA()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', MLPClassifier())])},\n",
        "          \n",
        "          \"robust_pca_ffs\":{\"KNeighborsClassifier\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", RobustScaler()), (\"pca\", PCA()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', KNeighborsClassifier())]), \n",
        "          \"RandomForestClassifier\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", RobustScaler()), (\"pca\", PCA()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', RandomForestClassifier())]),\n",
        "          \"SVC\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", RobustScaler()), (\"pca\", PCA()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c',SVC())]),\n",
        "          \"MLP\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", RobustScaler()), (\"pca\", PCA()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', MLPClassifier())])},\n",
        "          \n",
        "          \"minmax_ffs\":{\"KNeighborsClassifier\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", MinMaxScaler()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', KNeighborsClassifier())]),\n",
        "          \"RandomForestClassifier\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", MinMaxScaler()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', RandomForestClassifier())]),\n",
        "          \"SVC\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", MinMaxScaler()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c',SVC())]),\n",
        "          \"MLP\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", MinMaxScaler()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', MLPClassifier())])},\n",
        "          \n",
        "          \"robust_ffs\":{\"KNeighborsClassifier\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", RobustScaler()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', KNeighborsClassifier())]), \n",
        "          \"RandomForestClassifier\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", RobustScaler()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', RandomForestClassifier())]),\n",
        "          \"SVC\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", RobustScaler()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c',SVC())]),\n",
        "          \"MLP\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", RobustScaler()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', MLPClassifier())])}\n",
        "          }\n",
        "\n",
        "# Creating a dictionary of all parameters\n",
        "model_parameters = {\"KNeighborsClassifier\":{\"c__n_neighbors\":[x for x in range(1, 400)], \"c__weights\": [\"distance\", \"uniform\"]},\n",
        "                    \"RandomForestClassifier\":{\"c__n_estimators\":[x for x in range(2,100)], \"c__max_depth\":[x for x in range (1,60)], \"c__min_samples_leaf\": [x for x in range (20, 300)], \"c__random_state\" : [rs]},\n",
        "                    \"SVC\":{\"c__C\": [x for x in np.arange(0.00000001, 1, 0.00000001)], \"c__kernel\": [\"rbf\", \"poly\"], \"c__degree\":[x for x in range (2,4)], \"c__random_state\" : [rs]},\n",
        "                    \"MLP\": {\"c__hidden_layer_sizes\": [10,610,100], \"c__activation\": [\"relu\", \"logistic\", \"tanh\"], \"c__solver\": [\"sgd\", \"adam\", \"lbgfs\"], \n",
        "                            \"c__learning_rate\": [\"constant\", \"adaptive\"], \"c__max_iter\":[100], \"c__random_state\" : [rs]}\n",
        "                    }  \n",
        "\n",
        "param_with_PCA = {\"imp0__missing_values\": [0], \"pca__n_components\" : [nc], \"pca__random_state\": [rs], \"selector__k\": [k], \"sampling__sampling_strategy\": [\"minority\"], \"sampling__random_state\":[rs]}  \n",
        "param_without_PCA = {\"imp0__missing_values\": [0], \"selector__k\": [k], \"sampling__sampling_strategy\": [\"minority\"], \"sampling__random_state\":[rs]}      \n",
        "\n"
      ],
      "metadata": {
        "id": "rB8WQrT8nBlD"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going to write the functions to execute the Randomized search, return the needed results for the validation plots and plot the loss curves for the MLP classifier. Here we also directly calculate the outer cross-validation scores that will be dipslayed in section 5."
      ],
      "metadata": {
        "id": "aEPf-eqN5Nsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean up results and create dataframe with wanted results\n",
        "def get_dataframe_from_results(results, scorer):\n",
        "  \"\"\" This function makes sure that we can clean up the results that we get from the randomized search and leaves us with the necessary results\"\"\"\n",
        "  df_to_concat = [pd.DataFrame(results[\"params\"])]\n",
        "\n",
        "  for key in scorer:\n",
        "    df_to_concat.append(pd.DataFrame(results[f\"mean_train_{key}\"], columns=[f\"mean_train_{key}\"]))\n",
        "    df_to_concat.append(pd.DataFrame(results[f\"mean_test_{key}\"], columns=[f\"mean_test_{key}\"]))\n",
        "    df_to_concat.append(pd.DataFrame(results[f\"std_train_{key}\"], columns=[f\"std_train_{key}\"]))\n",
        "    df_to_concat.append(pd.DataFrame(results[f\"std_test_{key}\"], columns=[f\"std_test_{key}\"]))\n",
        "\n",
        "  return pd.concat(df_to_concat, axis=1)\n",
        "\n",
        "# Function to do a grid search on each model using the parameters in the model_parameter dictionary\n",
        "def parametertuning(CombinationName, Models, ModelParameters, n_splits, n_iter, random_state, scorer, refit, X, Y):\n",
        "  \"\"\" This function performs the parameter tuning by doing a randomized search with multiple parameters per model. It returns the results that are cleaned up, calculates the outer cross validation scores and plots the MLP loss curves\"\"\"\n",
        "  results = {} #create a dictionary to save results\n",
        "\n",
        "  for model_name, parameters in ModelParameters.items():\n",
        "    print(f\"Randomized searching {model_name}\")\n",
        "    model = Models[model_name]  # Find corresponding model in models dict\n",
        "\n",
        "    # Add constant parameters to the \n",
        "    if 'pca' in CombinationName:\n",
        "      params_complete = {**param_with_PCA, **parameters}\n",
        "    else:\n",
        "      params_complete = {**param_without_PCA, **parameters}\n",
        "\n",
        "    # Create the cross validation object for inner and outer cross validation\n",
        "    cv_inner = KFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
        "    cv_outer = KFold(n_splits=n_splits, random_state=random_state, shuffle=True)   \n",
        "    \n",
        "    # Perform a randomized search for the best parameters for the model using the test data and KFold\n",
        "    randomized_search = RandomizedSearchCV(model, params_complete, cv=cv_inner, n_iter=n_iter, n_jobs=-1, verbose=False, scoring=scorer, refit=refit, return_train_score=True, random_state = random_state) \n",
        "    randomized_search.fit(X, Y)\n",
        "\n",
        "    # Results of randomized search\n",
        "    best_estimator = randomized_search.best_estimator_ # Returns the model with the best parameters filled in based on refit scoring metric\n",
        "    best_params = randomized_search.best_params_ \n",
        "    best_score = randomized_search.best_score_\n",
        "    randomized_search_results = get_dataframe_from_results(randomized_search.cv_results_, scorer) # Extracts the necessary results \n",
        "\n",
        "    #Compute the outer cross validation scores to compare models (also the accuracy to see if we still are above the 85% in the assignment)\n",
        "    a_outer = np.mean(cross_val_score(best_estimator, X, Y, cv=cv_outer)) \n",
        "    f1_outer = np.mean(cross_val_score(best_estimator, X, Y, cv=cv_outer, scoring = scorer[\"F1\"])) \n",
        "    AUC_outer = np.mean(cross_val_score(best_estimator, X, Y, cv=cv_outer, scoring = scorer[\"AUC\"])) \n",
        "\n",
        "    # Add the results to the dictionary\n",
        "    results[model_name] = {\"BestEstimator\": best_estimator, \"BestParams\":best_params, \"BestScore\":best_score, \"Accuracy_outer\":a_outer, \"F1_outer\":f1_outer, \"AUC_outer\":AUC_outer, \"GSresults\": randomized_search_results}\n",
        "\n",
        "    if model_name is \"MLP\":\n",
        "      plt.plot(results[model_name][\"BestEstimator\"].named_steps[\"c\"].loss_curve_)\n",
        "      plt.title(f\"MLP loss curves\")\n",
        "      plt.legend()\n",
        "      plt.xlabel('Epochs')\n",
        "      plt.ylabel('Loss')\n",
        "\n",
        "  \n",
        "  return results"
      ],
      "metadata": {
        "id": "-zNkslru5CxC"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going write the function to plot the validation curves to estimate if we are giving the randomized search the correct ranges."
      ],
      "metadata": {
        "id": "7Fo3B9GtoHfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions to create validation plots\n",
        "\n",
        "#---------------Plot for KNeighborsClassifier---------------\n",
        "def plot_knn(results, scoring, combination_name, ModelParameters):\n",
        "  knn_df = results[\"KNeighborsClassifier\"][\"GSresults\"]\n",
        "  uni_knn_df = knn_df[knn_df[\"c__weights\"] == \"uniform\"]\n",
        "  dist_knn_df = knn_df[knn_df[\"c__weights\"] == \"distance\"]\n",
        "\n",
        "  fig, ax = plt.subplots(2,2) #create subplot\n",
        "  fig.suptitle(f\"KNN validation plots for {combination_name}\", fontsize=12)\n",
        "\n",
        "  # For uniform weight\n",
        "  grouped_n = uni_knn_df.groupby(by=[\"c__n_neighbors\"]).mean()\n",
        "  param_list = grouped_n.index.to_numpy()  # Pakt de parameters die op de x as gezet worden\n",
        "\n",
        "  for score, i in zip(scoring, range(0,2)): \n",
        "    ax[i,0].plot(param_list, grouped_n[f\"mean_train_{score}\"], label=\"Train\")  # plot the train F1 score\n",
        "    ax[i,0].plot(param_list, grouped_n[f\"mean_test_{score}\"], label=f\"Test\")  # plot the test F1 score\n",
        "    ax[i,0].set_title(f\"Uniform {score}\")\n",
        "    ax[i,0].set_xlabel(f\"n_neighbors\")\n",
        "    ax[i,0].set_ylabel(f\"{score}\")\n",
        "    ax[i,0].legend(loc='lower right')\n",
        "\n",
        "  # For distance weight\n",
        "  grouped_n = dist_knn_df.groupby(by=[\"c__n_neighbors\"]).mean()\n",
        "  param_list = grouped_n.index.to_numpy()  # Pakt de parameters die op de x as gezet worden\n",
        "\n",
        "  for score, i in zip(scoring, range(0,2)): \n",
        "    ax[i,1].plot(param_list, grouped_n[f\"mean_train_{score}\"], label=\"Train\")  # plot the train F1 score\n",
        "    ax[i,1].plot(param_list, grouped_n[f\"mean_test_{score}\"], label=\"Test\")  # plot the test F1 score\n",
        "    ax[i,1].set_title(f\"Distance {score}\")\n",
        "    ax[i,1].set_xlabel(f\"n_neighbors\")\n",
        "    ax[i,1].set_ylabel(f\"{score}\")\n",
        "    ax[i,1].legend(loc='lower right')\n",
        "\n",
        "  \n",
        "  for ax in fig.get_axes():\n",
        "    ax.label_outer()\n",
        "  \n",
        "#---------------Plot for SVC Classifier--------------- \n",
        "def plot_svc(results, scoring, combination_name, ModelParameters):\n",
        "  svc_df = results[\"SVC\"][\"GSresults\"]\n",
        "  rbf_svc_df = svc_df[svc_df[\"c__kernel\"]==\"rbf\"]\n",
        "  poly_svc_df = svc_df[svc_df[\"c__kernel\"]==\"poly\"]\n",
        "\n",
        "  fig, ax = plt.subplots(2,3) #create subplot\n",
        "  fig.suptitle(f\"SVC validation plots for {combination_name}\", fontsize=12)\n",
        "\n",
        "  # For RBF kernel\n",
        "  grouped_C = rbf_svc_df.groupby(by=[\"c__C\"]).mean()\n",
        "  param_list = grouped_C.index.to_numpy()  # Pakt de parameters die op de x as gezet worden\n",
        "\n",
        "  for score, i in zip(scoring, range(0,2)): \n",
        "    ax[i,0].plot(param_list, grouped_C[f\"mean_train_{score}\"], label=\"Train\")  # plot the train F1 score\n",
        "    ax[i,0].plot(param_list, grouped_C[f\"mean_test_{score}\"], label=\"Test\")  # plot the test F1 score\n",
        "    ax[i,0].set_title(f\"RBF {score}\")\n",
        "    ax[i,0].set_xlabel(f\"C\")\n",
        "    ax[i,0].set_ylabel(f\"{score}\")\n",
        "    ax[i,0].set_xscale('log')\n",
        "    ax[i,0].legend(loc='lower right')\n",
        "    \n",
        "  #For Poly kernels \n",
        "  for degree, i in zip(range(2,4), range(1,3)):\n",
        "    degree_svc_df = poly_svc_df[poly_svc_df[\"c__degree\"]==degree]\n",
        "    grouped_C = degree_svc_df.groupby(by=[\"c__C\"]).mean()\n",
        "    param_list = grouped_C.index.to_numpy()  # Pakt de parameters die op de x as gezet worden\n",
        "\n",
        "    for score, j in zip(scoring, range(0,2)): \n",
        "      ax[j,i].plot(param_list, grouped_C[f\"mean_train_{score}\"], label=\"Train\")  # plot the train F1 score\n",
        "      ax[j,i].plot(param_list, grouped_C[f\"mean_test_{score}\"], label=\"Test\")  # plot the test F1 score\n",
        "      ax[j,i].set_title(f\"Poly degree {degree} {score}\")\n",
        "      ax[j,i].set_xlabel(f\"C\")\n",
        "      ax[j,i].set_ylabel(f\"{score}\")\n",
        "      ax[j,i].set_xscale('log')\n",
        "      ax[j,i].legend(loc='lower right')\n",
        "\n",
        "  for ax in fig.get_axes():\n",
        "    ax.label_outer()\n",
        "       \n",
        "# ---------------Plot for RandomForestClassifier---------------\n",
        "def plot_randomforest(results, scoring, combination_name, ModelParameters):\n",
        "    forest_df = results[\"RandomForestClassifier\"][\"GSresults\"]\n",
        "\n",
        "    fig, ax = plt.subplots(2,3) #create subplot\n",
        "    fig.suptitle(f\"Decision tree validation plots for {combination_name}\", fontsize=12)\n",
        "    \n",
        "    for param, j in zip(ModelParameters[\"RandomForestClassifier\"], range(0,3)):\n",
        "      grouped = forest_df.groupby(by=[param]).mean()\n",
        "      param_list = grouped.index.to_numpy()  # Pakt de parameters die op de x as gezet worden\n",
        "      \n",
        "      for score, i in zip(scoring, range(0,2)): \n",
        "        ax[i,j].plot(param_list, grouped[f\"mean_train_{score}\"], label=\"Train\")  # plot the train F1 score\n",
        "        ax[i,j].plot(param_list, grouped[f\"mean_test_{score}\"], label=\"Test\")  # plot the test F1 score\n",
        "        ax[i,j].set_title(f\"{param.replace('c__', '')}\")\n",
        "        ax[i,j].set_xlabel(f\"{param.replace('c__', '')}\")\n",
        "        ax[i,j].set_ylabel(f\"{score}\")\n",
        "        ax[i,j].legend(loc='lower right')\n",
        "    \n",
        "    for ax in fig.get_axes():\n",
        "      ax.label_outer()"
      ],
      "metadata": {
        "id": "YS-1NeZunEfi"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beneath, we write the function to display the best scoring results for each model."
      ],
      "metadata": {
        "id": "DjUe1_wAoJpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot tabel with scoring metric and optimal paramater combination per scalor and selection\n",
        "def create_table_of_results(results_all_combinations, combination_name):\n",
        "  model_names = [] \n",
        "  best_params = []\n",
        "  best_scores = []\n",
        "  Mean_Accuracy = []\n",
        "  Mean_F1 = []\n",
        "  Mean_AUC = []\n",
        "\n",
        "  results = results_all_combinations[combination_name]\n",
        "\n",
        "  for model_name in results:\n",
        "\n",
        "    # Create column with model names\n",
        "    model_names.append(model_name)\n",
        "\n",
        "    # Create column with best params\n",
        "    best_param = results[model_name][\"BestParams\"]\n",
        "    best_params.append(best_param)\n",
        "\n",
        "    # Create column with best inner score\n",
        "    best_score = results[model_name][\"BestScore\"]\n",
        "    best_scores.append(best_score)\n",
        "    \n",
        "    # Create column with accuracy scores of outer cross validation of best estimator\n",
        "    mean_accuracy = results[model_name][\"Accuracy_outer\"]\n",
        "    Mean_Accuracy.append(mean_accuracy)\n",
        "\n",
        "    # Create column with accuracy scores of outer cross validation of best estimator\n",
        "    mean_f1 = results[model_name][\"F1_outer\"]\n",
        "    Mean_F1.append(mean_f1)\n",
        "    \n",
        "    # Create column with accuracy scores of outer cross validation of best estimator\n",
        "    mean_AUC = results[model_name][\"AUC_outer\"]\n",
        "    Mean_AUC.append(mean_AUC)\n",
        "\n",
        "  print(f\"Table of results for combination {combination_name}\")\n",
        "  display(pd.DataFrame({\"ModelName\":model_names, \"BestParams\":best_params, \"Best inner F1 score\": best_scores, \"Outer accuracy of BE\":Mean_Accuracy, \"Outer F1 of BE\":Mean_F1, \"Outer AUC of BE\":Mean_AUC}).style.hide(axis='index'))\n",
        "  print()"
      ],
      "metadata": {
        "id": "a3_dsVkEoKto"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we will call all functions written to tune all the parameters and plot the results for each combination of scaling, PCA and selection."
      ],
      "metadata": {
        "id": "IA5YZ10iM5OG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform training and plot MLP loss curve\n",
        "results_all_combinations = {}\n",
        "\n",
        "for combination_name, models in models_all_combinations.items():\n",
        "\n",
        "  #Create scorers\n",
        "  f1_scorer = make_scorer(f1_score, labels = None, average = 'binary', pos_label = 1) \n",
        "  scoring = {\"F1\":f1_scorer, \"AUC\":\"roc_auc\"} #Determine what type of scoring you want to use\n",
        "  \n",
        "  #Ignorme warnings from MLP\n",
        "  import warnings\n",
        "  warnings.filterwarnings('ignore') \n",
        "\n",
        "  # Perform randomized search\n",
        "  results = parametertuning(combination_name, models, model_parameters, 5, 50, None, scoring, \"F1\", x_train, y_train)\n",
        "  results_all_combinations[combination_name] = results"
      ],
      "metadata": {
        "id": "isqnuYlhM6Oz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "82320585-c9dc-4018-f452-32c1a42bb4f6"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Randomized searching KNeighborsClassifier\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-425d81aa100f>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m# Perform randomized search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparametertuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombination_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"F1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0mresults_all_combinations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcombination_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-70-a8644f463c0a>\u001b[0m in \u001b[0;36mparametertuning\u001b[0;34m(CombinationName, Models, ModelParameters, n_splits, n_iter, random_state, scorer, refit, X, Y)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m#Compute the outer cross validation scores to compare models (also the accuracy to see if we still are above the 85% in the assignment)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0ma_outer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv_outer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mf1_outer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv_outer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"F1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mAUC_outer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv_outer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"AUC\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    516\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    267\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/imblearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pipeline\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/imblearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             ):\n\u001b[0;32m--> 240\u001b[0;31m                 X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    241\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    891\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;31m# Call different fits for either full or truncated SVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"full\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"arpack\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"randomized\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_truncated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit_full\u001b[0;34m(self, X, n_components)\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m         \u001b[0;31m# flip eigenvectors' sign to enforce deterministic output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd_flip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/scipy/linalg/_decomp_svd.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;31m# perform decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     u, s, v, info = gesXd(a1, compute_uv=compute_uv, lwork=lwork,\n\u001b[0m\u001b[1;32m    128\u001b[0m                           full_matrices=full_matrices, overwrite_a=overwrite_a)\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create visualization of results \n",
        "plt.close('all') #make sure you class all windows before plotting to prevent plotting in the same figure\n",
        "\n",
        "for combination_name in models_all_combinations:\n",
        "\n",
        "  #Create all validation plots to show the relation between the scoring type and parameters for every combination\n",
        "  plot_knn(results_all_combinations[combination_name], scoring, combination_name, model_parameters)\n",
        "  plot_svc(results_all_combinations[combination_name], scoring, combination_name, model_parameters)\n",
        "  plot_randomforest(results_all_combinations[combination_name], scoring, combination_name, model_parameters)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "srThEOW1NPv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create table with results for every combination\n",
        "for combination_name in models_all_combinations:\n",
        "  create_table_of_results(results_all_combinations, combination_name)"
      ],
      "metadata": {
        "id": "S3yr30PFNSgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Modal comparison, final model and testing\n",
        "In this section we will determine our final model setup based on tables created with the different results. We will then test the final model setup in the pipeline on the test data. "
      ],
      "metadata": {
        "id": "sqnvS2MX2ozi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write final pipeline with chosen combinations\n",
        "def final_pipeline(x_train, y_train, results_all_combinations):\n",
        "    \"\"\"This function goes trough all the steps of the final model setup chosen in the previous section. Note: you need to run sections loading packages, 1, 2 and 4 in order to run this code\"\"\"\n",
        "    \n",
        "    # Display and set the steps of the pipeline\n",
        "    pipe = results_all_combinations[\"robust_ffs\"][\"SVC\"][\"BestEstimator\"]\n",
        "    print(f\"The steps of the final model look are {pipe}\\n\")\n",
        "\n",
        "    # Fit the pipeline on the training data \n",
        "    pipe.fit(x_train, y_train)\n",
        "\n",
        "    # Compute the scores of the model on the test data\n",
        "    y_pred= pipe.predict(x_test)\n",
        "    score_f1 = f1_score(y_test, y_pred)\n",
        "    score_AUC = roc_auc_score(y_test, y_pred)\n",
        "    score_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    return score_f1, score_AUC, score_accuracy\n",
        "\n",
        "#Execute pipeline to get the mean accuracy of the final model on the test data\n",
        "score_f1, score_AUC, score_accuracy = final_pipeline(x_train, y_train, results_all_combinations)\n",
        "print(f\"The scores of the final model are: f1 = {score_f1}, AUC = {score_AUC} and accuracy = {score_accuracy}\")\n"
      ],
      "metadata": {
        "id": "QAaZY7vY2vJg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
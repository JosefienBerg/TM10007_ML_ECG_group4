{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SXpaKwwGe5x"
      },
      "source": [
        "# TM10007 Group 4 -- ECG Data\n",
        "Lara de Bats (5022037), Josefien van den Berg (4663381), Merel Goossens (4856902), Amber Liqui Lung (4464168)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment we wil test certain classifiers in order to create a classifier model to distinguish between normal and abnormal ECG's. The code is divided in different sections that are explained in the corresponding method sections in our report."
      ],
      "metadata": {
        "id": "u0jPpiE5pyEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading packages\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.model_selection import KFold, RandomizedSearchCV, train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "# from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
        "from sklearn.svm import SVC #,LinearSVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import f1_score, make_scorer, accuracy_score, roc_auc_score\n",
        "\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "pd.set_option('display.max_rows', 30)"
      ],
      "metadata": {
        "id": "f-5Aw7sQpsDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EG78VSHfhNi"
      },
      "source": [
        "## 1. Data loading, initial split, exploring and cleaning\n",
        "\n",
        "Below we load the ECG data, convert it to a dataframe an clean the data.First we are going to start with loading the CS file into a dataframe. Then we are going to split the dataset into test data and training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiDn2Sk-VWqE",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Load CSV file into dataframe\n",
        "!git clone https://github.com/JosefienBerg/TM10007_ML_ECG_group4.git\n",
        "\n",
        "with zipfile.ZipFile('/content/TM10007_ML_ECG_group4/ecg/ecg_data.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/TM10007_ML_ECG_group4/ecg')\n",
        "\n",
        "df = pd.read_csv('/content/TM10007_ML_ECG_group4/ecg/ecg_data.csv', index_col=0)\n",
        "\n",
        "print(f'The number of samples: {len(df.index)}')\n",
        "print(f'The number of columns: {len(df.columns)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Underneath we explore our dataset"
      ],
      "metadata": {
        "id": "OaN6z_VynlYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many patients have a normal or abnormal ECG?\n",
        "count0= (df['label'] == 0).sum()\n",
        "count1= (df['label'] == 1).sum()\n",
        "print(f'There are {count0} patients with label 0')\n",
        "print(f'There are {count1} patients with label 1')\n",
        "# Since the majority of patients has no abnormalities we can conclude that label 0 is normal and label 1 is abnormal "
      ],
      "metadata": {
        "id": "6uzs9dYjnrHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will perform our initial split"
      ],
      "metadata": {
        "id": "ORtoTNM7LF54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the whole data set in a training and test set\n",
        "features = df.loc[:, df.columns !=\"label\"].to_numpy()\n",
        "labels = df[\"label\"].to_numpy()\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split (features, labels, train_size = 0.8, random_state = None) "
      ],
      "metadata": {
        "id": "7WHZbHF0LKCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets explore x_train"
      ],
      "metadata": {
        "id": "jhZt3EF9Wf3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at the size\n",
        "print(f\"The training set has {x_train.shape[0]} samples and {x_train.shape[1]} features with {y_train.shape[0]} labels\")\n",
        "print(f\"The test has {x_test.shape[0]} samples and {x_test.shape[1]} features with {y_test.shape[0]} labels\")\n",
        "\n",
        "# Look at the number of label 1 and label 0 in trainingset\n",
        "count0= (y_train == 0).sum()\n",
        "count1= (y_train == 1).sum()\n",
        "print(f'There are {count0} patients with label 0')\n",
        "print(f'There are {count1} patients with label 1')\n",
        "\n",
        "# Look at the distribution\n",
        "df = pd.DataFrame(x_train)\n",
        "display(df.describe())\n",
        "\n",
        "# Look at the type of data\n",
        "print(x_train.dtype)"
      ],
      "metadata": {
        "id": "g3RxneYdWh2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a last step, we want do delelte entire rows and columns with only zeros or NaN/None values as they can be seen as missing data."
      ],
      "metadata": {
        "id": "We8ePYjT1Qid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Delete rows and columns with only zero's or NaN's\n",
        "def delete_missing_rows(x, y):\n",
        "  concat = np.c_[x, y]\n",
        "\n",
        "  for row in x:\n",
        "    index = np.where((row == 0).all) or np.where((row == np.nan).all)\n",
        "    np.delete(concat, index, 0)\n",
        "  \n",
        "  x =  concat[:,:-1]\n",
        "  y = concat[:,-1] \n",
        "  return x, y\n",
        "\n",
        "x_train, y_train = delete_missing_rows(x_train, y_train)"
      ],
      "metadata": {
        "id": "zuqOeZIQ1FSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data preprocessing\n",
        "In this section we explore the data preprocessing steps before implementing them in the pipelines in section 3. Here we manually create the pipelines in order to test if the functions work and to determine the prinicpal components for PCA. \n"
      ],
      "metadata": {
        "id": "hbyr7EH1fUE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1 Imputation**; \n",
        "Underneath we try out the functions for the imputation steps. Datasets with missing values that are filled with placeholders such as NaN and None, can cause problems when using estimators. Therefore you want to perform imputation in order to create usable datasets. "
      ],
      "metadata": {
        "id": "iiggNmNvmsOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace nan's and zeros since the chance of a zero is very low due to only float types\n",
        "def replace_missing_values(x): \n",
        "  \"\"\"This function replaces all missing values\"\"\"\n",
        "  imp = SimpleImputer(missing_values = np.nan, strategy='mean')\n",
        "  imp.fit_transform(x, y=None) #removes entire columns if it contains only missing values\n",
        "\n",
        "  imp = SimpleImputer(missing_values = 0, strategy='mean')\n",
        "  imp.fit_transform(x, y=None) #removes entire columns if it contains only missing values\n",
        "  return x\n",
        "\n",
        "# Call functions to perform imputation steps and rename x_train\n",
        "x_train_imp = replace_missing_values(x_train)\n",
        "\n",
        "count0= (x_train_imp == 0).sum() \n",
        "countNaN = np.isnan(x_train_imp)[np.isnan(x_train_imp) == True].size\n",
        "print(f\"There are {count0} zeros and {countNaN} NaN values left in the dataset\")"
      ],
      "metadata": {
        "id": "nIf-5Y4zm3ND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2 Feature scaling**; Underneath we try the chosen scaling methods Robust and MinMax"
      ],
      "metadata": {
        "id": "qYnQ0Ml9oA1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Min-max scaling\n",
        "scaler_minmax = MinMaxScaler()\n",
        "x_scaled_minmax = scaler_minmax.fit_transform(x_train_imp)\n",
        "\n",
        "# 2. Robust scaling \n",
        "scaler_robust= RobustScaler()\n",
        "x_scaled_robust = scaler_robust.fit_transform(x_train_imp)\n",
        "\n",
        "x_scaled = {\"minmax\":x_scaled_minmax, \"robust\":x_scaled_robust}"
      ],
      "metadata": {
        "id": "7lPiglbIne_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3 Feature Extraction**; Now we are going to investigate the n_components for the possible usage of PCA"
      ],
      "metadata": {
        "id": "J9t4kREtoB0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA\n",
        "plt.close('all') #before plotting, close all old windows\n",
        "x_scaled_pca = {} #create dictionary to add all combinations of scaling combined with PCA to\n",
        "\n",
        "### Testing the plotting method (elbow curve) ###\n",
        "\n",
        "for key, x in x_scaled.items():\n",
        "\n",
        "    # Determine amount of Principal Components for pipeline with PCA and one of the scalers\n",
        "    pca_test = PCA(n_components =  None)\n",
        "    pca_test.fit(x)\n",
        "    variance = pca_test.explained_variance_ratio_\n",
        "    \n",
        "    # Bar plot for the pipeline with PCA and one of the scalers\n",
        "    g = plt.figure()\n",
        "    g.set_figwidth(14)\n",
        "    g.set_figheight(3)\n",
        "    variance=variance[:40] # determine range for visualisation  \n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.bar(range(1,len(variance )+1), variance, label = \"Individual Explained Variance\" )\n",
        "    plt.ylabel('Explained variance')\n",
        "    plt.xlabel('Component number')\n",
        "    plt.plot(range(1,len(variance )+1),\n",
        "         np.cumsum(variance),\n",
        "         c='red',\n",
        "         label=\"Cumulative Explained Variance\"\n",
        "          ) \n",
        "    plt.axvline(x=1, linewidth=1, color='green', alpha=0.5) #Kaiser's rule\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.title(f\"Bar Plot ({key} scaler with PCA)\")\n",
        "\n",
        "    # Scree plot the pipeline with PCA and one of the scalers\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.style.use(\"ggplot\") \n",
        "    plt.plot(variance[0:20], marker='o')\n",
        "    plt.axhline(y=1, linewidth=1, color='green', alpha=0.5) #Kaiser's rule\n",
        "    plt.xlabel(\"Eigenvalue number\")\n",
        "    plt.ylabel(\"Eigenvalue size\")\n",
        "    plt.title(f\"Scree Plot ({key} scaler with PCA)\")\n",
        "\n",
        "\n",
        "### Applying the final pca method ###\n",
        "\n",
        "for key, x in x_scaled.items():\n",
        "\n",
        "    pca_setting = PCA(n_components = 0.95) # threshold at 95% method\n",
        "    x_pca = pca_setting.fit_transform(x)\n",
        "    x_scaled_pca[f\"x_{key}_PCA\"] = x_pca\n",
        "\n",
        "    # plot to show treshold choice\n",
        "    pca_figure = PCA().fit(x)\n",
        "    plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    xi = np.arange(1, 662, step=1)\n",
        "    y = np.cumsum(pca_figure.explained_variance_ratio_)\n",
        "\n",
        "    plt.ylim(0.0,1.1)\n",
        "    plt.plot(xi, y, marker='o', linestyle='--', color='b')\n",
        "\n",
        "    plt.xlabel('Number of Components')\n",
        "    plt.xticks(np.arange(0, 661, step=50)) #change from 0-based array index to 1-based human-readable label\n",
        "    plt.ylabel('Cumulative variance (%)')\n",
        "    plt.title(f'The number of components needed to explain variance for scaling method {key}')\n",
        "\n",
        "    plt.axhline(y=0.95, color='r', linestyle='-')\n",
        "    plt.text(0.5, 0.9, '95% cut-off threshold', color = 'red', fontsize=16)\n",
        "\n",
        "    plt.axhline(y=0.85, color='r', linestyle='-')\n",
        "    plt.text(0.5, 0.8, '85% cut-off threshold', color = 'red', fontsize=16)\n",
        "\n",
        "    plt.axhline(y=0.75, color='r', linestyle='-')\n",
        "    plt.text(0.5, 0.7, '75% cut-off threshold', color = 'red', fontsize=16)\n",
        "\n",
        "    ax.grid(axis='x')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "nVCz7FgOnglf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4 Feature Selection**; For feature selection, the SelectKBest is used as explained in the method, this is as standard function directly implemented in the pipelines in the next section."
      ],
      "metadata": {
        "id": "mXpSLiVhLy0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.5 Creating pipelines**; this step is performed in the next section combined with directly adding the SMOTE and classifiers in the models_all_combinations dictionary."
      ],
      "metadata": {
        "id": "hmmzao3sp7mv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Classifiers: hyperparameter tuning and fitting\n",
        "In this section we will build a neural network and train our different clasiffiers (standard and the neural network) and compare their functionality in terms of F1 and AUC. The accuracy is also provided to see if we are still close to the 83% when randomly pikking. We will perform this for all possible combinations of scaling, selection and classification possibilities. This is also the section were we will tune our parameters.\n",
        "\n",
        "WARMING: Running this code takes approximately 2 hours. "
      ],
      "metadata": {
        "id": "UXm9Ps4sm-v-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we are going to set the pipelines and parameters we are going to tune and write the function for tuning. "
      ],
      "metadata": {
        "id": "jWntzXM0oGlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a dictionary of all proposed pipelines\n",
        "#GaussianNB and LinearSVC are removed from these dictionary's\n",
        "\n",
        "models_all_combinations = {\"minmax_pca_ffs\":{\"KNeighborsClassifier\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()), (\"scaler\", MinMaxScaler()), (\"pca\", PCA()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', KNeighborsClassifier())]),\n",
        "          \"RandomForestClassifier\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", MinMaxScaler()), (\"pca\", PCA()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', RandomForestClassifier())]),\n",
        "          \"SVC\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", MinMaxScaler()), (\"pca\", PCA()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c',SVC())]),\n",
        "          \"MLP\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", MinMaxScaler()), (\"pca\", PCA()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', MLPClassifier())])},\n",
        "          \n",
        "          \"robust_pca_ffs\":{\"KNeighborsClassifier\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", RobustScaler()), (\"pca\", PCA()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', KNeighborsClassifier())]), \n",
        "          \"RandomForestClassifier\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", RobustScaler()), (\"pca\", PCA()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', RandomForestClassifier())]),\n",
        "          \"SVC\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", RobustScaler()), (\"pca\", PCA()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c',SVC())]),\n",
        "          \"MLP\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", RobustScaler()), (\"pca\", PCA()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', MLPClassifier())])},\n",
        "          \n",
        "          \"minmax_ffs\":{\"KNeighborsClassifier\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", MinMaxScaler()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', KNeighborsClassifier())]),\n",
        "          \"RandomForestClassifier\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", MinMaxScaler()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', RandomForestClassifier())]),\n",
        "          \"SVC\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", MinMaxScaler()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c',SVC())]),\n",
        "          \"MLP\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", MinMaxScaler()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', MLPClassifier())])},\n",
        "          \n",
        "          \"robust_ffs\":{\"KNeighborsClassifier\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", RobustScaler()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', KNeighborsClassifier())]), \n",
        "          \"RandomForestClassifier\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", RobustScaler()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', RandomForestClassifier())]),\n",
        "          \"SVC\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", RobustScaler()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c',SVC())]),\n",
        "          \"MLP\": Pipeline([(\"impNaN\", SimpleImputer()), (\"imp0\", SimpleImputer()),(\"scaler\", RobustScaler()), (\"selector\", SelectKBest()), ('sampling', SMOTE()),('c', MLPClassifier())])}\n",
        "          }\n",
        "\n",
        "# Creating a dictionary of all parameters\n",
        "k = 10\n",
        "rs = None #Set random state, 42 only added in the last run\n",
        "nc = 0.95 #Set n_components for PCA\n",
        "\n",
        "model_parameters = {\"KNeighborsClassifier\":{\"c__n_neighbors\":[x for x in range(1, 400)], \"c__weights\": [\"distance\", \"uniform\"]},\n",
        "                    \"RandomForestClassifier\":{\"c__n_estimators\":[x for x in range(2,100)], \"c__max_depth\":[x for x in range (1,60)], \"c__min_samples_leaf\": [x for x in range (20, 300)], \"c__random_state\" : [rs]},\n",
        "                    \"SVC\":{\"c__C\": [x for x in np.arange(0.00000001, 1, 0.00000001)], \"c__kernel\": [\"rbf\", \"poly\"], \"c__degree\":[x for x in range (2,4)], \"c__random_state\" : [rs]},\n",
        "                    \"MLP\": {\"c__hidden_layer_sizes\": [10,300,600], \"c__activation\": [\"relu\", \"logistic\", \"tanh\"], \"c__solver\": [\"sgd\", \"adam\", \"lbgfs\"], \n",
        "                            \"c__learning_rate\": [\"constant\", \"adaptive\"], \"c__max_iter\":[100], \"c__random_state\" : [rs]}\n",
        "                    }  \n",
        "\n",
        "param_with_PCA = {\"imp0__missing_values\": [0], \"pca__n_components\" : [nc], \"pca__random_state\": [rs], \"selector__k\": [k], \"sampling__sampling_strategy\": [\"minority\"], \"sampling__random_state\":[rs]}  \n",
        "param_without_PCA = {\"imp0__missing_values\": [0], \"selector__k\": [k], \"sampling__sampling_strategy\": [\"minority\"], \"sampling__random_state\":[rs]}      \n",
        "\n"
      ],
      "metadata": {
        "id": "rB8WQrT8nBlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going to write the functions to execute the Randomized search, return the needed results for the validation plots and plot the loss curves for the MLP classifier. Here we also directly calculate the outer cross-validation scores that will be dipslayed in section 5."
      ],
      "metadata": {
        "id": "aEPf-eqN5Nsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean up results and create dataframe with wanted results\n",
        "def get_dataframe_from_results(results, scorer):\n",
        "  \"\"\" This function makes sure that we can clean up the results that we get from the randomized search and leaves us with the necessary results\"\"\"\n",
        "  df_to_concat = [pd.DataFrame(results[\"params\"])]\n",
        "\n",
        "  for key in scorer:\n",
        "    df_to_concat.append(pd.DataFrame(results[f\"mean_train_{key}\"], columns=[f\"mean_train_{key}\"]))\n",
        "    df_to_concat.append(pd.DataFrame(results[f\"mean_test_{key}\"], columns=[f\"mean_test_{key}\"]))\n",
        "    df_to_concat.append(pd.DataFrame(results[f\"std_train_{key}\"], columns=[f\"std_train_{key}\"]))\n",
        "    df_to_concat.append(pd.DataFrame(results[f\"std_test_{key}\"], columns=[f\"std_test_{key}\"]))\n",
        "\n",
        "  return pd.concat(df_to_concat, axis=1)\n",
        "\n",
        "# Function to do a grid search on each model using the parameters in the model_parameter dictionary\n",
        "def parametertuning(CombinationName, Models, ModelParameters, n_splits, n_iter, random_state, scorer, refit, X, Y):\n",
        "  \"\"\" This function performs the parameter tuning by doing a randomized search with multiple parameters per model. It returns the results that are cleaned up, calculates the outer cross validation scores and plots the MLP loss curves\"\"\"\n",
        "  results = {} #create a dictionary to save results\n",
        "\n",
        "  for model_name, parameters in ModelParameters.items():\n",
        "    print(f\"Randomized searching {model_name}\")\n",
        "    model = Models[model_name]  # Find corresponding model in models dict\n",
        "\n",
        "    # Add constant parameters to the \n",
        "    if 'pca' in CombinationName:\n",
        "      params_complete = {**param_with_PCA, **parameters}\n",
        "    else:\n",
        "      params_complete = {**param_without_PCA, **parameters}\n",
        "\n",
        "    # Create the cross validation object for inner and outer cross validation\n",
        "    cv_inner = KFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
        "    cv_outer = KFold(n_splits=n_splits, random_state=random_state, shuffle=True)   \n",
        "    \n",
        "    # Perform a randomized search for the best parameters for the model using the test data and KFold\n",
        "    randomized_search = RandomizedSearchCV(model, params_complete, cv=cv_inner, n_iter=n_iter, n_jobs=-1, verbose=False, scoring=scorer, refit=refit, return_train_score=True, random_state = random_state) \n",
        "    randomized_search.fit(X, Y)\n",
        "\n",
        "    # Results of randomized search\n",
        "    best_estimator = randomized_search.best_estimator_ # Returns the model with the best parameters filled in based on refit scoring metric\n",
        "    best_params = randomized_search.best_params_ \n",
        "    best_score = randomized_search.best_score_\n",
        "    randomized_search_results = get_dataframe_from_results(randomized_search.cv_results_, scorer) # Extracts the necessary results \n",
        "\n",
        "    #Compute the outer cross validation scores to compare models (also the accuracy to see if we still are above the 85% in the assignment)\n",
        "    a_outer = np.mean(cross_val_score(best_estimator, X, Y, cv=cv_outer)) \n",
        "    f1_outer = np.mean(cross_val_score(best_estimator, X, Y, cv=cv_outer, scoring = scorer[\"F1\"])) \n",
        "    AUC_outer = np.mean(cross_val_score(best_estimator, X, Y, cv=cv_outer, scoring = scorer[\"AUC\"])) \n",
        "\n",
        "    # Add the results to the dictionary\n",
        "    results[model_name] = {\"BestEstimator\": best_estimator, \"BestParams\":best_params, \"BestScore\":best_score, \"Accuracy_outer\":a_outer, \"F1_outer\":f1_outer, \"AUC_outer\":AUC_outer, \"GSresults\": randomized_search_results}\n",
        "  \n",
        "  return results"
      ],
      "metadata": {
        "id": "-zNkslru5CxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going write the function to plot the validation curves to estimate if we are giving the randomized search the correct ranges."
      ],
      "metadata": {
        "id": "7Fo3B9GtoHfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions to create validation plots and loss curve\n",
        "\n",
        "#---------------Plot for KNeighborsClassifier---------------\n",
        "def plot_knn(results, scoring, combination_name, ModelParameters):\n",
        "  \"\"\"This function plots the validation curves for all pipeline combinations for the KNeighborsClassifier\"\"\"\n",
        "  knn_df = results[\"KNeighborsClassifier\"][\"GSresults\"]\n",
        "  uni_knn_df = knn_df[knn_df[\"c__weights\"] == \"uniform\"]\n",
        "  dist_knn_df = knn_df[knn_df[\"c__weights\"] == \"distance\"]\n",
        "\n",
        "  fig, ax = plt.subplots(2,2) #create subplot\n",
        "  fig.suptitle(f\"KNN validation plots for {combination_name}\", fontsize=12)\n",
        "\n",
        "  # For uniform weight\n",
        "  grouped_n = uni_knn_df.groupby(by=[\"c__n_neighbors\"]).mean()\n",
        "  param_list = grouped_n.index.to_numpy()  # Pakt de parameters die op de x as gezet worden\n",
        "\n",
        "  for score, i in zip(scoring, range(0,2)): \n",
        "    ax[i,0].plot(param_list, grouped_n[f\"mean_train_{score}\"], label=\"Train\")  # plot the train F1 score\n",
        "    ax[i,0].plot(param_list, grouped_n[f\"mean_test_{score}\"], label=f\"Test\")  # plot the test F1 score\n",
        "    ax[i,0].set_title(f\"Uniform {score}\")\n",
        "    ax[i,0].set_xlabel(f\"n_neighbors\")\n",
        "    ax[i,0].set_ylabel(f\"{score}\")\n",
        "    ax[i,0].legend(loc='lower right')\n",
        "\n",
        "  # For distance weight\n",
        "  grouped_n = dist_knn_df.groupby(by=[\"c__n_neighbors\"]).mean()\n",
        "  param_list = grouped_n.index.to_numpy()  # Pakt de parameters die op de x as gezet worden\n",
        "\n",
        "  for score, i in zip(scoring, range(0,2)): \n",
        "    ax[i,1].plot(param_list, grouped_n[f\"mean_train_{score}\"], label=\"Train\")  # plot the train F1 score\n",
        "    ax[i,1].plot(param_list, grouped_n[f\"mean_test_{score}\"], label=\"Test\")  # plot the test F1 score\n",
        "    ax[i,1].set_title(f\"Distance {score}\")\n",
        "    ax[i,1].set_xlabel(f\"n_neighbors\")\n",
        "    ax[i,1].set_ylabel(f\"{score}\")\n",
        "    ax[i,1].legend(loc='lower right')\n",
        "\n",
        "  \n",
        "  for ax in fig.get_axes():\n",
        "    ax.label_outer()\n",
        "  \n",
        "#---------------Plot for SVC Classifier--------------- \n",
        "def plot_svc(results, scoring, combination_name, ModelParameters):\n",
        "  \"\"\"This function plots the validation curves for all pipeline combinations for the SVCClassifier\"\"\"\n",
        "  svc_df = results[\"SVC\"][\"GSresults\"]\n",
        "  rbf_svc_df = svc_df[svc_df[\"c__kernel\"]==\"rbf\"]\n",
        "  poly_svc_df = svc_df[svc_df[\"c__kernel\"]==\"poly\"]\n",
        "\n",
        "  fig, ax = plt.subplots(2,3) #create subplot\n",
        "  fig.suptitle(f\"SVC validation plots for {combination_name}\", fontsize=12)\n",
        "\n",
        "  # For RBF kernel\n",
        "  grouped_C = rbf_svc_df.groupby(by=[\"c__C\"]).mean()\n",
        "  param_list = grouped_C.index.to_numpy()  # Pakt de parameters die op de x as gezet worden\n",
        "\n",
        "  for score, i in zip(scoring, range(0,2)): \n",
        "    ax[i,0].plot(param_list, grouped_C[f\"mean_train_{score}\"], label=\"Train\")  # plot the train F1 score\n",
        "    ax[i,0].plot(param_list, grouped_C[f\"mean_test_{score}\"], label=\"Test\")  # plot the test F1 score\n",
        "    ax[i,0].set_title(f\"RBF {score}\")\n",
        "    ax[i,0].set_xlabel(f\"C\")\n",
        "    ax[i,0].set_ylabel(f\"{score}\")\n",
        "    ax[i,0].set_xscale('log')\n",
        "    ax[i,0].legend(loc='lower right')\n",
        "    \n",
        "  #For Poly kernels \n",
        "  for degree, i in zip(range(2,4), range(1,3)):\n",
        "    degree_svc_df = poly_svc_df[poly_svc_df[\"c__degree\"]==degree]\n",
        "    grouped_C = degree_svc_df.groupby(by=[\"c__C\"]).mean()\n",
        "    param_list = grouped_C.index.to_numpy()  # Pakt de parameters die op de x as gezet worden\n",
        "\n",
        "    for score, j in zip(scoring, range(0,2)): \n",
        "      ax[j,i].plot(param_list, grouped_C[f\"mean_train_{score}\"], label=\"Train\")  # plot the train F1 score\n",
        "      ax[j,i].plot(param_list, grouped_C[f\"mean_test_{score}\"], label=\"Test\")  # plot the test F1 score\n",
        "      ax[j,i].set_title(f\"Poly degree {degree} {score}\")\n",
        "      ax[j,i].set_xlabel(f\"C\")\n",
        "      ax[j,i].set_ylabel(f\"{score}\")\n",
        "      ax[j,i].set_xscale('log')\n",
        "      ax[j,i].legend(loc='lower right')\n",
        "\n",
        "  for ax in fig.get_axes():\n",
        "    ax.label_outer()\n",
        "       \n",
        "# ---------------Plot for RandomForestClassifier---------------\n",
        "def plot_randomforest(results, scoring, combination_name, ModelParameters):\n",
        "  \"\"\"This function plots the validation curves for all pipeline combinations for the RandomForestClassifier\"\"\"\n",
        "  forest_df = results[\"RandomForestClassifier\"][\"GSresults\"]\n",
        "\n",
        "  fig, ax = plt.subplots(2,3) #create subplot\n",
        "  fig.suptitle(f\"Decision tree validation plots for {combination_name}\", fontsize=12)\n",
        "  \n",
        "  for param, j in zip(ModelParameters[\"RandomForestClassifier\"], range(0,3)):\n",
        "    grouped = forest_df.groupby(by=[param]).mean()\n",
        "    param_list = grouped.index.to_numpy()  # Pakt de parameters die op de x as gezet worden\n",
        "    \n",
        "    for score, i in zip(scoring, range(0,2)): \n",
        "      ax[i,j].plot(param_list, grouped[f\"mean_train_{score}\"], label=\"Train\")  # plot the train F1 score\n",
        "      ax[i,j].plot(param_list, grouped[f\"mean_test_{score}\"], label=\"Test\")  # plot the test F1 score\n",
        "      ax[i,j].set_title(f\"{param.replace('c__', '')}\")\n",
        "      ax[i,j].set_xlabel(f\"{param.replace('c__', '')}\")\n",
        "      ax[i,j].set_ylabel(f\"{score}\")\n",
        "      ax[i,j].legend(loc='lower right')\n",
        "  \n",
        "  for ax in fig.get_axes():\n",
        "    ax.label_outer()\n",
        "\n",
        "# ---------------Plot for MLP loss curve---------------\n",
        "def plot_MLP(CombinationName, results):\n",
        "  fig.suptitle(f\"MLP loss curves\", fontsize=12)\n",
        "  fig, ax = plt.subplots(1,4)\n",
        "\n",
        "  for i in range(0,4):\n",
        "    ax[0,i].plot(results[\"MLP\"][\"BestEstimator\"].named_steps[\"c\"].loss_curve_, label=f'{CombinationName}')\n",
        "    ax[0,i].set_title(f\"MLP loss curves\")\n",
        "    ax[0,i].legend(loc='upper right')\n",
        "    ax[0,i].set_xlabel('Epochs')\n",
        "    ax[0,i].set_ylabel('Loss')"
      ],
      "metadata": {
        "id": "YS-1NeZunEfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we will call all functions written to tune all the parameters and plot the results for each combination of scaling, PCA and selection."
      ],
      "metadata": {
        "id": "IA5YZ10iM5OG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform training and plot MLP loss curve\n",
        "results_all_combinations = {}\n",
        "\n",
        "for combination_name, models in models_all_combinations.items():\n",
        "\n",
        "  #Create scorers\n",
        "  f1_scorer = make_scorer(f1_score, labels = None, average = 'binary', pos_label = 1) \n",
        "  scoring = {\"F1\":f1_scorer, \"AUC\":\"roc_auc\"} #Determine what type of scoring you want to use\n",
        "  \n",
        "  #Ignorme warnings from MLP\n",
        "  import warnings\n",
        "  warnings.filterwarnings('ignore') \n",
        "\n",
        "  # Perform randomized search\n",
        "  results = parametertuning(combination_name, models, model_parameters, 5, 50, None, scoring, \"F1\", x_train, y_train)\n",
        "  results_all_combinations[combination_name] = results"
      ],
      "metadata": {
        "id": "isqnuYlhM6Oz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d204c418-4264-499e-b8bb-abb96edb1d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Randomized searching KNeighborsClassifier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create validation curves\n",
        "plt.close('all') #make sure you class all windows before plotting to prevent plotting in the same figure\n",
        "\n",
        "for combination_name in models_all_combinations:\n",
        "\n",
        "  #Create all validation plots to show the relation between the scoring type and parameters for every combination\n",
        "  plot_knn(results_all_combinations[combination_name], scoring, combination_name, model_parameters)\n",
        "  plot_svc(results_all_combinations[combination_name], scoring, combination_name, model_parameters)\n",
        "  plot_randomforest(results_all_combinations[combination_name], scoring, combination_name, model_parameters)\n",
        "  plot_MLP(results_all_combinations[combination_name], combination_name)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "srThEOW1NPv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Modal comparison, final model and testing\n",
        "In this section we will determine our final model setup based on tables created with the different results. We will then test the final model setup in the pipeline on the test data. If you want to only see the final pipeline with results, the code is written in such a way that you only need to to run section 1 and the last code block of this section."
      ],
      "metadata": {
        "id": "sqnvS2MX2ozi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beneath, we write and execute the function to display the best scoring results for each model."
      ],
      "metadata": {
        "id": "DjUe1_wAoJpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write function to plot table. \n",
        "def create_table_of_results(results_all_combinations, combination_name):\n",
        "  \"\"\"This function plots a table containing the model names, best paramete combination, best inner F1 score that is used to choose the BestEstimator and the outer cross validation scores\"\"\"\n",
        "  model_names = [] \n",
        "  best_params = []\n",
        "  best_scores = []\n",
        "  Mean_Accuracy = []\n",
        "  Mean_F1 = []\n",
        "  Mean_AUC = []\n",
        "\n",
        "  results = results_all_combinations[combination_name]\n",
        "\n",
        "  for model_name in results:\n",
        "\n",
        "    # Create column with model names\n",
        "    model_names.append(model_name)\n",
        "\n",
        "    # Create column with best params\n",
        "    best_param = results[model_name][\"BestParams\"]\n",
        "    best_params.append(best_param)\n",
        "\n",
        "    # Create column with best inner score\n",
        "    best_score = results[model_name][\"BestScore\"]\n",
        "    best_scores.append(best_score)\n",
        "    \n",
        "    # Create column with accuracy scores of outer cross validation of best estimator\n",
        "    mean_accuracy = results[model_name][\"Accuracy_outer\"]\n",
        "    Mean_Accuracy.append(mean_accuracy)\n",
        "\n",
        "    # Create column with accuracy scores of outer cross validation of best estimator\n",
        "    mean_f1 = results[model_name][\"F1_outer\"]\n",
        "    Mean_F1.append(mean_f1)\n",
        "    \n",
        "    # Create column with accuracy scores of outer cross validation of best estimator\n",
        "    mean_AUC = results[model_name][\"AUC_outer\"]\n",
        "    Mean_AUC.append(mean_AUC)\n",
        "\n",
        "  print(f\"Table of results for combination {combination_name}\")\n",
        "  display(pd.DataFrame({\"ModelName\":model_names, \"BestParams\":best_params, \"Best inner F1 score\": best_scores, \"Outer accuracy of BE\":Mean_Accuracy, \"Outer F1 of BE\":Mean_F1, \"Outer AUC of BE\":Mean_AUC}).style.hide(axis='index'))\n",
        "  print()\n",
        "\n",
        "# Create table with results for every combination\n",
        "for combination_name in models_all_combinations:\n",
        "  create_table_of_results(results_all_combinations, combination_name)"
      ],
      "metadata": {
        "id": "1mX9RLQEtV0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write final pipeline with chosen combinations\n",
        "def final_pipeline(x_train, y_train, results_all_combinations):\n",
        "    \"\"\"This function goes trough all the steps of the final model setup chosen in the previous section. Note: you need to run sections loading packages, 1 and 3 in order to run this code\"\"\"\n",
        "    \n",
        "    # Display and set the steps of the pipeline\n",
        "    pipe = results_all_combinations[\"robust_ffs\"][\"SVC\"][\"BestEstimator\"]\n",
        "    print(f\"The steps of the final model look are {pipe}\\n\")\n",
        "\n",
        "    # Fit the pipeline on the training data \n",
        "    pipe.fit(x_train, y_train)\n",
        "\n",
        "    # Compute the scores of the model on the test data\n",
        "    y_pred= pipe.predict(x_test)\n",
        "    score_f1 = f1_score(y_test, y_pred)\n",
        "    score_AUC = roc_auc_score(y_test, y_pred)\n",
        "    score_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    return score_f1, score_AUC, score_accuracy, y_pred\n",
        "\n",
        "#Execute pipeline to get the mean accuracy of the final model on the test data\n",
        "score_f1, score_AUC, score_accuracy, y_pred = final_pipeline(x_train, y_train, results_all_combinations)\n",
        "print(f\"The scores of the final model are: f1 = {score_f1}, AUC = {score_AUC} and accuracy = {score_accuracy}\")\n",
        "\n",
        "#Create confusion matrix for discussion\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QAaZY7vY2vJg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
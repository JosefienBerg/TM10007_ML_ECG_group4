{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SXpaKwwGe5x"
      },
      "source": [
        "# TM10007 Group 4 -- ECG Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hallo tekst"
      ],
      "metadata": {
        "id": "u0jPpiE5pyEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading packages\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "import numpy as np\n",
        "\n"
      ],
      "metadata": {
        "id": "f-5Aw7sQpsDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EG78VSHfhNi"
      },
      "source": [
        "## 1. Data loading, cleaning and visualization\n",
        "\n",
        "Below we load the ECG data, convert it to a dataframe an clean the data.First we are going to start with loading the CS file into a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiDn2Sk-VWqE",

        

   

      "source": [
        "# Initial split\n",
        "\n",
        "# packages \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split the whole data set in a training and test set\n",
        "col = \"label\"\n",
        "x_train, x_test, y_train, y_test = train_test_split (data.loc[:, data.columns !=col], data.loc[:, \"label\"].values, train_size = 0.8, random_state = 42) \n",
        "print(x_train)"
      ],


      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# # 1. Standardization of the data\n",
        "# scaler = StandardScaler()\n",
        "# x_scaled = scaler.fit_transform(x_train)\n",
        "\n",
        "# 2. Min-max scaling\n",
        "scaler = MinMaxScaler()\n",
        "x_scaled = scaler.fit_transform(x_train)\n",
        "\n",
        "# # 3. Robust scaling\n",
        "# scaler = RobustScaler()\n",
        "# x_scaled = scaler.fit_transform(x_train)\n",
        "\n",
        "print(x_scaled.round(2))\n",
        "print(type(x_scaled))\n",
        "\n",
        "# # Provides information on scaled data\n",
        "# df_x_scaled = pd.DataFrame(x_scaled)\n",
        "# summary_stats = df_x_scaled.describe()\n",
        "# print(summary_stats)\n",
        "\n",
        "summary_stats = x_train.describe()\n",
        "print(summary_stats)"
      ],


      "source": [
        "There are multiple ways to scale features. Since the feature distribution is unclear, we will use normalization."
      ],
